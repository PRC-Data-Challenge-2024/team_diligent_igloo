{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The notebook \"hyperparam_opt\" focuses on optimizing model hyperparameters, specifically using the XGBRegressor model from the xgboost library. The goal is to identify the best hyperparameters that improve model performance, particularly by minimizing the root mean squared error (RMSE).We perform hyperparameter tuning using a random sampling approach and Stratified KFold cross-validation to identify optimal hyperparameter settings, assess model performance, and determine feature importance. It iterates over different combinations of hyperparameters, records the results, and stores them in a DataFrame for further analysis. We then run the best set of hyperparams against the final test set to get the final Estimated TOWs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_df(corr_df, file_name=None, show=True):\n",
    "    \"\"\"\n",
    "    corr_df must be a pandas.DataFrame().corr() object\n",
    "    Automatically adjusts the plot size based on the number of variables, ensuring all squares in the heatmap are evenly sized.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.figure_factory as ff\n",
    "    from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "    labels = corr_df.columns\n",
    "    n_vars = len(labels)\n",
    "\n",
    "    # Define square size and compute overall figure dimensions\n",
    "    square_size = 30  # Size of each heatmap cell in pixels\n",
    "    heatmap_size = square_size * n_vars  # Total size for heatmap portion\n",
    "    dendro_size = 200  # Fixed size for dendrogram (can be adjusted)\n",
    "\n",
    "    # Total figure dimensions\n",
    "    fig_width = min(1000, heatmap_size + dendro_size + 200)  # 200 for margins and color bar\n",
    "    fig_height = min(950, heatmap_size + dendro_size + 150)  # 150 for margins and labels\n",
    "\n",
    "    # Initialize figure by creating upper dendrogram\n",
    "    fig = ff.create_dendrogram(\n",
    "        corr_df.values, \n",
    "        labels=labels,\n",
    "        orientation='bottom', \n",
    "        linkagefun=lambda x: linkage(\n",
    "            x, method=\"ward\", optimal_ordering=True\n",
    "        )\n",
    "    )\n",
    "    for i in range(len(fig['data'])):\n",
    "        fig['data'][i]['yaxis'] = 'y2'\n",
    "\n",
    "    # Create Side Dendrogram\n",
    "    dendro_side = ff.create_dendrogram(\n",
    "        corr_df.values, \n",
    "        labels=labels,\n",
    "        orientation='right',\n",
    "        linkagefun=lambda x: linkage(\n",
    "            x, method=\"ward\", optimal_ordering=True\n",
    "        )\n",
    "    )\n",
    "    for i in range(len(dendro_side['data'])):\n",
    "        dendro_side['data'][i]['xaxis'] = 'x2'\n",
    "\n",
    "    # Add Side Dendrogram Data to Figure\n",
    "    for data in dendro_side['data']:\n",
    "        fig.add_trace(data)\n",
    "\n",
    "    # Create Heatmap\n",
    "    dendro_leaves = dendro_side['layout']['yaxis']['ticktext']\n",
    "\n",
    "    heatmap = [\n",
    "        go.Heatmap(\n",
    "            x=dendro_leaves,\n",
    "            y=dendro_leaves,\n",
    "            z=corr_df.loc[dendro_leaves, dendro_leaves],\n",
    "            colorscale='tempo',\n",
    "            zmin=-1,\n",
    "            zmax=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    heatmap[0]['x'] = fig['layout']['xaxis']['tickvals']\n",
    "    heatmap[0]['y'] = fig['layout']['xaxis']['tickvals']\n",
    "\n",
    "    # Add Heatmap Data to Figure\n",
    "    for data in heatmap:\n",
    "        fig.add_trace(data)\n",
    "\n",
    "    # Adjust layout to dynamically resize based on the number of variables\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=fig_width,\n",
    "        height=fig_height,\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(l=15, r=15, t=15, b=15),  # Adjust margins dynamically\n",
    "    )\n",
    "\n",
    "    # Edit xaxis (bottom dendrogram)\n",
    "    fig.update_layout(xaxis={\n",
    "        'domain': [.15, 1],  # Dynamically adjust domain based on space\n",
    "        'mirror': False,\n",
    "        'showgrid': False,\n",
    "        'showline': False,\n",
    "        'zeroline': False,\n",
    "        'ticktext': dendro_leaves,\n",
    "        'ticks': \"\",\n",
    "    })\n",
    "    # Edit xaxis2 (side dendrogram axis)\n",
    "    fig.update_layout(xaxis2={\n",
    "        'domain': [0, .15],  # Adjust domain for side dendrogram\n",
    "        'mirror': False,\n",
    "        'showgrid': False,\n",
    "        'showline': False,\n",
    "        'zeroline': False,\n",
    "        'showticklabels': False,\n",
    "        'ticks': \"\"\n",
    "    })\n",
    "    \n",
    "    # Edit yaxis (heatmap and side dendrogram)\n",
    "    fig.update_layout(yaxis={\n",
    "        'domain': [0, .85],  # Dynamically adjust domain based on space\n",
    "        'mirror': False,\n",
    "        'showgrid': False,\n",
    "        'showline': False,\n",
    "        'zeroline': False,\n",
    "        'showticklabels': False,\n",
    "        'ticks': \"\",\n",
    "        'ticktext': dendro_leaves,\n",
    "        'tickvals': np.array(range(n_vars)) * 10 + 5\n",
    "    })\n",
    "    # Edit yaxis2 (top dendrogram axis)\n",
    "    fig.update_layout(yaxis2={\n",
    "        'domain': [.85, 1],  # Adjust domain for top dendrogram\n",
    "        'mirror': False,\n",
    "        'showgrid': False,\n",
    "        'showline': False,\n",
    "        'zeroline': False,\n",
    "        'showticklabels': False,\n",
    "        'ticks': \"\"\n",
    "    })\n",
    "\n",
    "    if file_name:\n",
    "        fig.write_html(file_name)\n",
    "    \n",
    "    # Plot!\n",
    "    if show:\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_corr_columns(corr_df, corr_th=.7):\n",
    "\n",
    "    \"\"\"\n",
    "    Identify columns in a correlation DataFrame that are below a specified correlation threshold.\n",
    "    \n",
    "    This function iteratively removes columns from the correlation matrix that have high correlation \n",
    "    values with other columns, based on the provided threshold. It ensures the remaining columns \n",
    "    have pairwise correlations below the specified threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    corr_df : pd.DataFrame\n",
    "        A DataFrame containing pairwise correlation values for features.\n",
    "    corr_th : float, optional\n",
    "        The correlation threshold for filtering (default is 0.7). Columns with correlations \n",
    "        above this threshold will be progressively removed until no pairs exceed it.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    np.array\n",
    "        Array of column names that are below the specified correlation threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    mezo_corr = corr_df.abs()\n",
    "    while (mezo_corr >= corr_th).sum().sum() > len(mezo_corr):\n",
    "        columns_over_corr_counts = (mezo_corr >= corr_th).sum()\n",
    "        if columns_over_corr_counts.sum() > len(mezo_corr):\n",
    "            col_to_remove = mezo_corr.columns[\n",
    "                columns_over_corr_counts.argmax()\n",
    "            ]\n",
    "            mezo_corr = mezo_corr.drop(columns=col_to_remove).drop(index=col_to_remove)\n",
    "\n",
    "    low_corr_columns = np.array(mezo_corr.columns)\n",
    "    \n",
    "    return low_corr_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>col_sample</th>\n",
       "      <th>corr_th</th>\n",
       "      <th>n_quantiles</th>\n",
       "      <th>RMSE_avg</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>time_s</th>\n",
       "      <th>rank_1_feature</th>\n",
       "      <th>rank_1_importance</th>\n",
       "      <th>rank_2_feature</th>\n",
       "      <th>rank_2_importance</th>\n",
       "      <th>rank_3_feature</th>\n",
       "      <th>rank_3_importance</th>\n",
       "      <th>rank_4_feature</th>\n",
       "      <th>rank_4_importance</th>\n",
       "      <th>rank_5_feature</th>\n",
       "      <th>rank_5_importance</th>\n",
       "      <th>rank_6_feature</th>\n",
       "      <th>rank_6_importance</th>\n",
       "      <th>rank_7_feature</th>\n",
       "      <th>rank_7_importance</th>\n",
       "      <th>rank_8_feature</th>\n",
       "      <th>rank_8_importance</th>\n",
       "      <th>rank_9_feature</th>\n",
       "      <th>rank_9_importance</th>\n",
       "      <th>rank_10_feature</th>\n",
       "      <th>rank_10_importance</th>\n",
       "      <th>rank_11_feature</th>\n",
       "      <th>rank_11_importance</th>\n",
       "      <th>rank_12_feature</th>\n",
       "      <th>rank_12_importance</th>\n",
       "      <th>rank_13_feature</th>\n",
       "      <th>rank_13_importance</th>\n",
       "      <th>rank_14_feature</th>\n",
       "      <th>rank_14_importance</th>\n",
       "      <th>rank_15_feature</th>\n",
       "      <th>rank_15_importance</th>\n",
       "      <th>rank_16_feature</th>\n",
       "      <th>rank_16_importance</th>\n",
       "      <th>rank_17_feature</th>\n",
       "      <th>rank_17_importance</th>\n",
       "      <th>rank_18_feature</th>\n",
       "      <th>rank_18_importance</th>\n",
       "      <th>rank_19_feature</th>\n",
       "      <th>rank_19_importance</th>\n",
       "      <th>rank_20_feature</th>\n",
       "      <th>rank_20_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [learning_rate, n_estimators, max_depth, col_sample, corr_th, n_quantiles, RMSE_avg, RMSE_std, time_s, rank_1_feature, rank_1_importance, rank_2_feature, rank_2_importance, rank_3_feature, rank_3_importance, rank_4_feature, rank_4_importance, rank_5_feature, rank_5_importance, rank_6_feature, rank_6_importance, rank_7_feature, rank_7_importance, rank_8_feature, rank_8_importance, rank_9_feature, rank_9_importance, rank_10_feature, rank_10_importance, rank_11_feature, rank_11_importance, rank_12_feature, rank_12_importance, rank_13_feature, rank_13_importance, rank_14_feature, rank_14_importance, rank_15_feature, rank_15_importance, rank_16_feature, rank_16_importance, rank_17_feature, rank_17_importance, rank_18_feature, rank_18_importance, rank_19_feature, rank_19_importance, rank_20_feature, rank_20_importance]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9807b8917b45bd94081666c6095e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b83952af504e47a73f94f50debef23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "learning_rate=0.07, n_estimators=300, max_depth=17, col_sample=0.95, corr_th=0.76, n_quantiles=3: 0it [00:00, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_n = 20 # Number of most important features to store\n",
    "n_folds = 5 # Reduced from 20 for quicker processing\n",
    "\n",
    "results_file_name = \"results_df_v2.pkl\"\n",
    "\n",
    "# Try to load existing results DataFrame, if it doesn't exist, initialize an empty DataFrame\n",
    "try:\n",
    "    results_df = joblib.load(results_file_name)\n",
    "except:\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"learning_rate\", \"n_estimators\", \"max_depth\", \"col_sample\", \"corr_th\", \"n_quantiles\", \"RMSE_avg\", \"RMSE_std\", \"time_s\"\n",
    "        ] + np.array([[f\"rank_{i+1}_feature\", f\"rank_{i+1}_importance\"] for i in range(top_n)]).flatten().tolist()\n",
    "    )\n",
    "display(results_df.sort_values(by=\"RMSE_avg\").head())\n",
    "\n",
    "# Define parameter ranges for filtering correlations and model hyperparameters\n",
    "min_corr_th, max_corr_th = 0.7, 1\n",
    "min_max_depth, max_max_depth = 3, 20\n",
    "min_col_sample, max_col_sample = 0.45, 1\n",
    "min_n_estimators, max_n_estimators = 100, 1000\n",
    "min_learning_rate, max_learning_rate = 1e-2, 5e-1\n",
    "\n",
    "# Create possible values for each hyperparameter based on the defined ranges\n",
    "possible_n_quantiles = [3, 5, 11]\n",
    "possible_max_depths = np.arange(min_max_depth, max_max_depth+1, 1)\n",
    "possible_corr_ths = np.arange(min_corr_th, max_corr_th, .01).round(2)\n",
    "possible_col_samples = np.arange(min_col_sample, max_col_sample+0.01, 0.05).round(2)\n",
    "possible_n_estimators = np.arange(min_n_estimators//10, max_n_estimators//10+1, 1) * 10\n",
    "possible_learning_rates = np.concatenate([\n",
    "    np.arange(min_learning_rate*10**exp, min_learning_rate*10**(exp+1), min_learning_rate*10**exp)\n",
    "    for exp in range(int(np.log10(1 / min_learning_rate)))\n",
    "]).round(3)\n",
    "possible_learning_rates = possible_learning_rates[possible_learning_rates <= max_learning_rate]\n",
    "\n",
    "# Perform random walk to select parameters and run model hyperparameter optimization\n",
    "np.random.seed(42)\n",
    "for i in tqdm(range(200)):\n",
    "    # Randomly sample hyperparameters from the specified ranges\n",
    "    corr_th = np.random.choice(possible_corr_ths)\n",
    "    max_depth = np.random.choice(possible_max_depths)\n",
    "    col_sample = np.random.choice(possible_col_samples)\n",
    "    n_quantiles = np.random.choice(possible_n_quantiles)\n",
    "    n_estimators = np.random.choice(possible_n_estimators)\n",
    "    learning_rate = np.random.choice(possible_learning_rates)\n",
    "    \n",
    "    # Create a descriptor string and check if the configuration was used before\n",
    "    desc = f\"learning_rate={learning_rate}, n_estimators={n_estimators}, max_depth={max_depth}, \" + \\\n",
    "           f\"col_sample={col_sample}, corr_th={corr_th}, n_quantiles={n_quantiles}\"\n",
    "    \n",
    "    hyperset = [learning_rate, n_estimators, max_depth, col_sample, corr_th, n_quantiles]\n",
    "    result_done = ((results_df.values[:, :len(hyperset)] == hyperset).sum(axis=1) == len(hyperset)).sum()\n",
    "    \n",
    "    # Only proceed if this hyperparameter set hasn't been tested\n",
    "    if not result_done:   \n",
    "\n",
    "        # Load preprocessed data\n",
    "        X_y_df = joblib.load(f\"X_y_df_cha_nq={n_quantiles}.pkl\")\n",
    "\n",
    "        # Set target and feature columns\n",
    "        target_column = X_y_df.columns[-1]\n",
    "        feature_columns = X_y_df.columns[2:-1]\n",
    "        cat_features = [column for column in feature_columns if \"airline_\" in column]\n",
    "        num_features = sorted(list(set(feature_columns).difference(set(cat_features))))\n",
    "        \n",
    "        # Compute or load correlation data to exclude highly correlated features\n",
    "        if f\"X_y_cha_nq={n_quantiles}_corr_df.pkl\" not in os.listdir(os.getcwd()):\n",
    "            corr_df = X_y_df[num_features].corr()\n",
    "            joblib.dump(corr_df, f\"X_y_cha_nq={n_quantiles}_corr_df.pkl\")\n",
    "        else:\n",
    "            corr_df = joblib.load(f\"X_y_cha_nq={n_quantiles}_corr_df.pkl\")\n",
    "        \n",
    "        # Create feature matrix and target vector\n",
    "        X = X_y_df[feature_columns]\n",
    "        y = X_y_df[target_column]\n",
    "        \n",
    "        # Select columns based on correlation threshold and add categorical features\n",
    "        low_corr_columns = get_low_corr_columns(corr_df, corr_th).tolist() + cat_features\n",
    "        \n",
    "        # Move data to GPU for performance optimization\n",
    "        y_cuda = torch.from_numpy(y.values).to(\"cuda\")\n",
    "        X_cuda = torch.from_numpy(X[low_corr_columns].values).to(\"cuda\")\n",
    "        \n",
    "        rmses = []\n",
    "        # Stratified K-Folds cross-validator\n",
    "        splitter = StratifiedKFold(n_splits=n_folds, random_state=42, shuffle=True)\n",
    "        \n",
    "        # Training loop for each fold\n",
    "        for train_idx, test_idx in tqdm(splitter.split(y.values, y.values), leave=False, desc=desc):\n",
    "            X_train, X_test = X_cuda[train_idx], X_cuda[test_idx]\n",
    "            y_train, y_test = y_cuda[train_idx], y.values[test_idx]\n",
    "            \n",
    "            # Train model on each fold\n",
    "            reg = XGBR(\n",
    "                n_jobs=-1,\n",
    "                device=\"cuda\",\n",
    "                random_state=42,\n",
    "                max_depth=max_depth, \n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                colsample_bytree=col_sample,\n",
    "                colsample_bylevel=col_sample,\n",
    "                colsample_bynode=col_sample,\n",
    "            ).fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate model performance\n",
    "            y_pred = reg.predict(X_test)\n",
    "            rmses.append(rmse(y_test, y_pred))\n",
    "\n",
    "        mean_rmse = np.mean(rmses)\n",
    "        stdv_rmse = np.std(rmses, ddof=0)\n",
    "        \n",
    "        # Append results to DataFrame if it’s empty\n",
    "        if not len(results_df):\n",
    "            results_df = pd.DataFrame(\n",
    "                data=[\n",
    "                    hyperset + [mean_rmse, stdv_rmse] + np.repeat(np.nan, len(results_df.columns) - len(hyperset) - 2).tolist()\n",
    "                ],\n",
    "                columns=results_df.columns\n",
    "            )\n",
    "            str_cols = [column for column in results_df.columns if \"feature\" in column]\n",
    "            results_df[str_cols] = results_df[str_cols].astype(str)\n",
    "        \n",
    "        # Else add a new row for the current results\n",
    "        else:\n",
    "            row_df = pd.DataFrame(\n",
    "                data=[\n",
    "                    hyperset + [mean_rmse, stdv_rmse] + np.repeat(np.nan, len(results_df.columns) - len(hyperset) - 2).tolist()\n",
    "                ],\n",
    "                columns=results_df.columns\n",
    "            )\n",
    "            str_cols = [column for column in row_df.columns if \"feature\" in column]\n",
    "            row_df[str_cols] = row_df[str_cols].astype(str)\n",
    "            \n",
    "            results_df = pd.concat(\n",
    "                [results_df, row_df],\n",
    "                ignore_index=True,\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "        # Measure full dataset training time\n",
    "        start = time.time()\n",
    "        reg = XGBR(\n",
    "            n_jobs=-1,\n",
    "            device=\"cuda\",\n",
    "            random_state=42,\n",
    "            max_depth=max_depth, \n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            colsample_bytree=col_sample,\n",
    "            colsample_bylevel=col_sample,\n",
    "            colsample_bynode=col_sample,\n",
    "        ).fit(X_cuda, y_cuda)\n",
    "        stop = time.time()\n",
    "        \n",
    "        # Update timing in results DataFrame\n",
    "        results_df.loc[\n",
    "            (results_df.values[:, :len(hyperset)] == hyperset).sum(axis=1) == len(hyperset),\n",
    "            \"time_s\"\n",
    "        ] = stop - start\n",
    "        \n",
    "        # Record feature importances\n",
    "        feature_order_idxs = np.argsort(reg.feature_importances_)[::-1]\n",
    "        for i in range(top_n):\n",
    "            feature_name = (X[low_corr_columns].columns[feature_order_idxs])[i]\n",
    "            feature_importance = (reg.feature_importances_[feature_order_idxs])[i]\n",
    "        \n",
    "            results_df.loc[\n",
    "                (results_df.values[:, :len(hyperset)] == hyperset).sum(axis=1) == len(hyperset),\n",
    "                f\"rank_{i+1}_feature\"\n",
    "            ] = feature_name\n",
    "        \n",
    "            results_df.loc[\n",
    "                (results_df.values[:, :len(hyperset)] == hyperset).sum(axis=1) == len(hyperset),\n",
    "                 f\"rank_{i+1}_importance\"\n",
    "            ] = feature_importance\n",
    "            \n",
    "        # Save updated results DataFrame\n",
    "        joblib.dump(results_df, results_file_name)\n",
    "        \n",
    "    # Retrieve and display current RMSE metrics and feature importances\n",
    "    mean_rmse, stdv_rmse = results_df.loc[\n",
    "        (results_df.values[:, :len(hyperset)] == hyperset).sum(axis=1) == len(hyperset),\n",
    "        [\"RMSE_avg\", \"RMSE_std\"]\n",
    "    ].values.flatten()\n",
    "    \n",
    "    features = results_df.loc[\n",
    "        (results_df.values[:, :len(hyperset)] == hyperset).sum(axis=1) == len(hyperset),\n",
    "        [column for column in results_df.columns if \"feature\" in column]\n",
    "    ].values.flatten()\n",
    "\n",
    "            \n",
    "        joblib.dump(results_df, results_file_name)\n",
    "        \n",
    "    mean_rmse, stdv_rmse = results_df.loc[\n",
    "        (results_df.values[:, :len(hyperset)]==hyperset).sum(axis=1)==len(hyperset),\n",
    "        [\"RMSE_avg\", \"RMSE_std\"]\n",
    "    ].values.flatten()\n",
    "    \n",
    "    features = results_df.loc[\n",
    "        (results_df.values[:, :len(hyperset)]==hyperset).sum(axis=1)==len(hyperset),\n",
    "        [column for column in results_df.columns if \"feature\" in column]\n",
    "    ].values.flatten()\n",
    "    \n",
    "    importances = results_df.loc[\n",
    "        (results_df.values[:, :len(hyperset)]==hyperset).sum(axis=1)==len(hyperset),\n",
    "        [column for column in results_df.columns if \"importance\" in column]\n",
    "    ].values.flatten()\n",
    "    \n",
    "    print(\n",
    "        f\"Average RMSE: {mean_rmse:.2f}\"+\\\n",
    "        f\"\\nStandard Deviation: {stdv_rmse:.2f}\"\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, dpi=100, figsize=(16,5))\n",
    "    plt.clf()\n",
    "    plt.barh(\n",
    "        y=features[::-1],\n",
    "        width=importances[::-1]\n",
    "    )\n",
    "    plt.title(desc)\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f26c3f43bc74925accc952e3d83eb01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "16f606df97a34b53976dea3d08bea485": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1e3d12496b2b4e36aa77b8e35a8a0438": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0f26c3f43bc74925accc952e3d83eb01",
       "style": "IPY_MODEL_5646c97149d44c1d8090a9214d92cc78",
       "value": " 0/200 [00:00&lt;?, ?it/s]"
      }
     },
     "2f1e623207314278a106ff85f766e101": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2fad12239b6b403185ee60ee26609166": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "44522d59529c4f27863d35efd9bdec23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9e0209f41e284f90bfa316f3991d3221",
       "style": "IPY_MODEL_7df14f47768c498baa4e5f3ddb98af04",
       "value": "  0%"
      }
     },
     "52bd5483a0714a1e9440410c98fc90f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_f4bcb40d4fcf4d87a96ce4d1012562da",
       "max": 200,
       "style": "IPY_MODEL_2fad12239b6b403185ee60ee26609166"
      }
     },
     "5646c97149d44c1d8090a9214d92cc78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56dd6a58d0e6405399ddad46e218a2f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9c6f8c8aecae44a88b942b1295587e31",
       "style": "IPY_MODEL_2f1e623207314278a106ff85f766e101",
       "value": " 1/? [08:53&lt;00:00, 533.51s/it]"
      }
     },
     "7df14f47768c498baa4e5f3ddb98af04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "862821f2bce24ea9b86ec116ec6df111": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9c6f8c8aecae44a88b942b1295587e31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e0209f41e284f90bfa316f3991d3221": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a834b54b508f4ff9b9ec42f7c058af6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "20px"
      }
     },
     "bd429ae1ded9408692d604d57c8ca105": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c6b83952af504e47a73f94f50debef23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_db37d5bbdccd4f21ad1e18bdce39fbd2",
        "IPY_MODEL_e7c1dbfd940b467fbd15dbad5c0f4370",
        "IPY_MODEL_56dd6a58d0e6405399ddad46e218a2f5"
       ],
       "layout": "IPY_MODEL_e762e3eb56414d38996bf3faaee55588"
      }
     },
     "db37d5bbdccd4f21ad1e18bdce39fbd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_16f606df97a34b53976dea3d08bea485",
       "style": "IPY_MODEL_f55b277e40b04273b286a4a14a7a9a6a",
       "value": "learning_rate=0.07, n_estimators=300, max_depth=17, col_sample=0.95, corr_th=0.76, n_quantiles=3: "
      }
     },
     "e762e3eb56414d38996bf3faaee55588": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e7c1dbfd940b467fbd15dbad5c0f4370": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "info",
       "layout": "IPY_MODEL_a834b54b508f4ff9b9ec42f7c058af6a",
       "max": 1,
       "style": "IPY_MODEL_862821f2bce24ea9b86ec116ec6df111",
       "value": 1
      }
     },
     "f4bcb40d4fcf4d87a96ce4d1012562da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f55b277e40b04273b286a4a14a7a9a6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fd9807b8917b45bd94081666c6095e36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_44522d59529c4f27863d35efd9bdec23",
        "IPY_MODEL_52bd5483a0714a1e9440410c98fc90f0",
        "IPY_MODEL_1e3d12496b2b4e36aa77b8e35a8a0438"
       ],
       "layout": "IPY_MODEL_bd429ae1ded9408692d604d57c8ca105"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
