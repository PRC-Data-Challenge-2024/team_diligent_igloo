{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Disclaimer\n",
    "\n",
    "We, the authors of this work, hereby disclaim all copyright interest in the notebooks and code submitted as part of the PRC Data Challenge 2024, which predicts Estimated Aircraft Take-Off Mass. \n",
    "\n",
    "Signed, \n",
    "\n",
    "Antonio P. Barata <br>\n",
    "Bernard Bronmans <br>\n",
    "Victor Ciulei <br>\n",
    "\n",
    "28.10.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This code defines a comprehensive series of functions for cleaning, transforming, and analyzing flight data. It begins by identifying and filtering highly correlated features from the dataset using a recursive approach, where it removes the most correlated columns iteratively until all remaining features meet a specified correlation threshold. The code also includes utility functions to create cyclical representations of time data, allowing for the incorporation of temporal variables like day, week, month, and year using trigonometric transformations that convert time into cyclic x-y coordinates on a unit circle. Additionally, it defines phase segmentation for flights by detecting shifts in the flight phase and grouping related segments, which later supports in-depth phase-specific analyses.\n",
    "\n",
    "To analyze flight statistics, the code provides methods to calculate various statistical measures, including means, standard deviations, quantiles, and coefficients of variation for critical flight metrics. These statistics are further segmented based on flight phases to capture variability within specific segments and between phase changes. Using quantiles and variability metrics, the code extracts detailed insights for each flight’s distinct phases, emphasizing inter-segment variability to capture shifts in flight behavior over time. Finally, it includes batch processing, allowing for efficient calculation of these metrics across multiple flights by iterating through flight files, compiling each flight’s data into a unified summary DataFrame. We include emissions data, engine performance data and engine noise data in the additional features for aircraft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import joblib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "from traffic.core import Flight\n",
    "from traffic.core import Traffic\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "runway_df = pd.read_csv(\"runways.csv\")\n",
    "airports_df = pd.read_csv(\"airports.csv\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "flights_folder = os.path.join(os.getcwd(), \"flightDfs\")\n",
    "cha_df = pd.read_csv(os.path.join(data_folder, \"challenge_set.csv\"))\n",
    "sub_df = pd.read_csv(os.path.join(data_folder, \"submission_set.csv\"))\n",
    "final_sub_df = pd.read_csv(os.path.join(data_folder, \"final_submission_set.csv\"))\n",
    "\n",
    "#identify the useful flight IDs\n",
    "usable_flight_ids = list(set(cha_df.flight_id.unique()).union(\n",
    "    set(sub_df.flight_id.unique()).union(\n",
    "        set(final_sub_df.flight_id.unique())\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_corr_columns(corr_df, corr_th=.90):\n",
    "    \"\"\"\n",
    "    Identifies and removes highly correlated columns from a correlation matrix.\n",
    "\n",
    "    Parameters:\n",
    "    corr_df (pd.DataFrame): The input correlation matrix.\n",
    "    corr_th (float, optional): The correlation threshold for considering columns as highly correlated. Default is 0.90.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two arrays:\n",
    "        - non_corr_columns (np.array): Array of columns that are not highly correlated.\n",
    "        - corr_columns (np.array): Array of columns that were removed due to high correlation.\n",
    "    \"\"\"\n",
    "    mezo_corr = corr_df.abs()\n",
    "    while (mezo_corr >= corr_th).sum().sum() > len(mezo_corr):\n",
    "        columns_over_corr_counts = (mezo_corr >= corr_th).sum()\n",
    "        if columns_over_corr_counts.sum() > len(mezo_corr):\n",
    "            col_to_remove = mezo_corr.columns[\n",
    "                columns_over_corr_counts.argmax()\n",
    "            ]\n",
    "            mezo_corr = mezo_corr.drop(columns=col_to_remove).drop(index=col_to_remove)\n",
    "\n",
    "    corr_columns = np.array(set(corr_df.columns).difference(set(mezo_corr.columns)))\n",
    "    non_corr_columns = np.array(mezo_corr.columns)\n",
    "    \n",
    "    return non_corr_columns, corr_columns\n",
    "\n",
    "\n",
    "def is_leap_year(year):\n",
    "    \"\"\"\n",
    "    Determines if a given year is a leap year.\n",
    "\n",
    "    Parameters:\n",
    "    year (int): The year to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the year is a leap year, False otherwise.\n",
    "    \"\"\"\n",
    "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
    "\n",
    "\n",
    "def day_cyclic_values(timestamps):\n",
    "    \"\"\"\n",
    "    Computes cyclic values (x, y) for time of day using a unit circle.\n",
    "\n",
    "    Parameters:\n",
    "    timestamps (pd.Series): Timestamps for which to compute the cyclic values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays representing the x and y coordinates on a unit circle.\n",
    "    \"\"\"\n",
    "    second_of_day = timestamps.dt.hour * 3600 + timestamps.dt.minute * 60 + timestamps.dt.second\n",
    "    second_of_day_normalized = second_of_day / 86400.0  # 86400 seconds in a day\n",
    "    x_day = np.cos(2 * np.pi * second_of_day_normalized)\n",
    "    y_day = np.sin(2 * np.pi * second_of_day_normalized)\n",
    "    return x_day, y_day\n",
    "\n",
    "\n",
    "def week_cyclic_values(timestamps):\n",
    "    \"\"\"\n",
    "    Computes cyclic values (x, y) for the day of the week using a unit circle.\n",
    "\n",
    "    Parameters:\n",
    "    timestamps (pd.Series): Timestamps for which to compute the cyclic values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays representing the x and y coordinates on a unit circle.\n",
    "    \"\"\"\n",
    "    day_of_week = timestamps.dt.dayofweek\n",
    "    second_of_day = timestamps.dt.hour * 3600 + timestamps.dt.minute * 60 + timestamps.dt.second\n",
    "    day_of_week_normalized = day_of_week / 7.0\n",
    "    second_of_day_normalized = second_of_day / 86400.0  # 86400 seconds in a day\n",
    "    week_fraction = day_of_week_normalized + second_of_day_normalized / 7.0\n",
    "    x_week = np.cos(2 * np.pi * week_fraction)\n",
    "    y_week = np.sin(2 * np.pi * week_fraction)\n",
    "    return x_week, y_week\n",
    "\n",
    "\n",
    "def month_cyclic_values(timestamps):\n",
    "    \"\"\"\n",
    "    Computes cyclic values (x, y) for the day of the month using a unit circle.\n",
    "\n",
    "    Parameters:\n",
    "    timestamps (pd.Series): Timestamps for which to compute the cyclic values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays representing the x and y coordinates on a unit circle.\n",
    "    \"\"\"\n",
    "    day_of_month = timestamps.dt.day\n",
    "    second_of_day = timestamps.dt.hour * 3600 + timestamps.dt.minute * 60 + timestamps.dt.second\n",
    "    days_in_month = timestamps.dt.days_in_month\n",
    "    day_of_month_normalized = day_of_month / days_in_month\n",
    "    second_of_day_normalized = second_of_day / 86400.0  # 86400 seconds in a day\n",
    "    month_fraction = day_of_month_normalized + second_of_day_normalized / days_in_month\n",
    "    x_month = np.cos(2 * np.pi * month_fraction)\n",
    "    y_month = np.sin(2 * np.pi * month_fraction)\n",
    "    return x_month, y_month\n",
    "\n",
    "\n",
    "def year_cyclic_values(timestamps):\n",
    "    \"\"\"\n",
    "    Computes cyclic values (x, y) for the day of the year using a unit circle, accounting for leap years.\n",
    "\n",
    "    Parameters:\n",
    "    timestamps (pd.Series): Timestamps for which to compute the cyclic values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays representing the x and y coordinates on a unit circle.\n",
    "    \"\"\"\n",
    "    year = timestamps.dt.year\n",
    "    day_of_year = timestamps.dt.dayofyear\n",
    "    second_of_day = timestamps.dt.hour * 3600 + timestamps.dt.minute * 60 + timestamps.dt.second\n",
    "    days_in_year = year.apply(lambda y: 366 if is_leap_year(y) else 365)\n",
    "    day_of_year_normalized = day_of_year / days_in_year\n",
    "    second_of_day_normalized = second_of_day / 86400.0  # 86400 seconds in a day\n",
    "    year_fraction = day_of_year_normalized + second_of_day_normalized / days_in_year\n",
    "    x_year = np.cos(2 * np.pi * year_fraction)\n",
    "    y_year = np.sin(2 * np.pi * year_fraction)\n",
    "    return x_year, y_year\n",
    "\n",
    "def generate_phase_segments(df):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into segments where the 'phase' column changes.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame containing a 'phase' column.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are phase values and values are lists of indices for each segment.\n",
    "    \"\"\"\n",
    "    df = df[[\"phase\"]].copy()\n",
    "    # Create a shifted phase column to detect phase changes\n",
    "    df['shifted_phase'] = df['phase'].shift(1)\n",
    "\n",
    "    # Mark where phase changes (start of a new segment)\n",
    "    df['segment'] = (df['phase'] != df['shifted_phase']).cumsum()\n",
    "\n",
    "    # Initialize the dictionary to store the segments\n",
    "    phase_segments = {}\n",
    "\n",
    "    # Use groupby to group by phase and segment and collect indices\n",
    "    for (phase, segment), group in df.groupby(['phase', 'segment']):\n",
    "        if phase not in phase_segments:\n",
    "            phase_segments[phase] = []\n",
    "        phase_segments[phase].append(np.array(list(group.index)))\n",
    "\n",
    "    return phase_segments\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Compute the great-circle distance between two points on the Earth's surface using NumPy, in meters.\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Radius of earth in meters (6371 km converted to meters)\n",
    "    r = 6371000\n",
    "    return c * r\n",
    "\n",
    "def calculate_distances(df):\n",
    "    # Shift the lat/lon columns to create the \"previous\" row\n",
    "    lat1 = df['latitude_adep'].values\n",
    "    lon1 = df['longitude_adep'].values\n",
    "    lat2 = df['latitude_ades'].values\n",
    "    lon2 = df['longitude_ades'].values\n",
    "    \n",
    "    # Compute the distance between consecutive points\n",
    "    distances = haversine_np(lon1, lat1, lon2, lat2)\n",
    "\n",
    "    # Add distances to DataFrame in meters\n",
    "    return distances\n",
    "\n",
    "def get_flight_stats_row(flight_df=\"df\", n_quantiles=5, stats_columns=\"list\"):\n",
    "    \"\"\"\n",
    "    Computes statistics (mean, std, quantiles) and cyclic variability for flight data.\n",
    "\n",
    "    Parameters:\n",
    "    flight_df (pd.DataFrame): DataFrame containing flight data.\n",
    "    n_quantiles (int): Number of quantiles to compute.\n",
    "    stats_columns (list): List of column names to compute statistics on.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A series of computed statistics including mean, standard deviation, quantiles, \n",
    "               and variability within quantile bins.\n",
    "    \"\"\"\n",
    "    means = flight_df[stats_columns].mean()\n",
    "    stdvs = flight_df[stats_columns].std(ddof=0)\n",
    "    quants = flight_df[stats_columns].quantile(q=np.linspace(0,1,n_quantiles).round(2))\n",
    "    total_distance = pd.Series(flight_df.distance.sum(), index=[\"total_distance\"])\n",
    "    # rearranging\n",
    "    means.index = [\"mean_\" + ind for ind in means.index]\n",
    "    stdvs.index = [\"stdv_\" + ind for ind in stdvs.index]\n",
    "    quants = quants.stack().to_frame().T\n",
    "    quants.columns = [f\"{col[0]}_{col[1]}\" for col in quants.columns]\n",
    "    quants = quants.loc[0]\n",
    "    \n",
    "    # Step 1: Prepare dictionaries to hold standard deviations and normalized standard deviations\n",
    "    quantile_stdvs_dict = {}\n",
    "    normalized_stdvs_dict = {}\n",
    "    \n",
    "    # Step 2: Loop over each stat_column\n",
    "    for column in stats_columns:\n",
    "        # Get the quantile values for the current column, ensuring duplicates are dropped\n",
    "        quantile_bins = flight_df[column].quantile(q=np.linspace(0, 1, n_quantiles)).values\n",
    "        unique_quantile_bins = np.unique(quantile_bins)  # Remove duplicate quantile values\n",
    "        \n",
    "        # Digitize the data points into the unique quantile bins\n",
    "        bin_labels = np.arange(1, len(unique_quantile_bins))  # Adjust labels for the available unique bins\n",
    "        binned_data = pd.cut(flight_df[column], bins=unique_quantile_bins, labels=bin_labels, include_lowest=True)\n",
    "        \n",
    "        # Compute the standard deviation for each unique bin\n",
    "        if len(unique_quantile_bins)==1:\n",
    "            for i in range(1, n_quantiles):\n",
    "                quantile_stdvs_dict[f'{column}_quant_{i}_stdv'] = stdvs[f'stdv_{column}']\n",
    "        else:\n",
    "            for i in range(1, len(unique_quantile_bins)):\n",
    "                bin_stdv = flight_df[column][binned_data == i].std()\n",
    "                quantile_stdvs_dict[f'{column}_quant_{i}_stdv'] = bin_stdv\n",
    "        \n",
    "            # If there are fewer quantiles than desired, duplicate the last bin's standard deviation        \n",
    "            last_stdv = quantile_stdvs_dict[f'{column}_quant_{len(unique_quantile_bins)-1}_stdv']\n",
    "            for i in range(len(unique_quantile_bins), n_quantiles):\n",
    "                quantile_stdvs_dict[f'{column}_quant_{i}_stdv'] = last_stdv\n",
    "        \n",
    "        # Normalize the standard deviations by dividing by the absolute mean (handling division by zero)\n",
    "        mean_value = abs(means[f'mean_{column}'])\n",
    "        for i in range(1, n_quantiles):\n",
    "            stdv_col_name = f'{column}_quant_{i}_stdv'\n",
    "            stdv_value = quantile_stdvs_dict[stdv_col_name]\n",
    "            normalized_value = stdv_value / mean_value if mean_value != 0 else -1\n",
    "            normalized_stdvs_dict[f'{column}_quant_{i}_norm_stdv'] = normalized_value\n",
    "    \n",
    "    # Step 3: Convert the dictionaries to DataFrames\n",
    "    quantile_stdvs = pd.DataFrame(quantile_stdvs_dict, index=[0])\n",
    "    normalized_stdvs = pd.DataFrame(normalized_stdvs_dict, index=[0])\n",
    "    \n",
    "    # concatenating everything\n",
    "    flight_stats_row = pd.concat([\n",
    "        means, stdvs, quants, total_distance, quantile_stdvs.squeeze(), normalized_stdvs.squeeze()\n",
    "    ])\n",
    "\n",
    "    return flight_stats_row\n",
    "\n",
    "def get_flight_stats_df(flight_df=\"df\", n_quantiles=5, stats_columns=\"list\", possible_phases=\"list\"):\n",
    "    \"\"\"\n",
    "    Computes flight statistics for each phase and generates a DataFrame with the results.\n",
    "\n",
    "    Parameters:\n",
    "    flight_df (pd.DataFrame): DataFrame containing flight data.\n",
    "    n_quantiles (int): Number of quantiles to compute.\n",
    "    stats_columns (list): List of column names to compute statistics on.\n",
    "    possible_phases (list): List of possible flight phases to consider.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing statistics for each flight and phase.\n",
    "    \"\"\"\n",
    "    global_flight_stats = get_flight_stats_row(\n",
    "        flight_df=flight_df, \n",
    "        n_quantiles=n_quantiles,\n",
    "        stats_columns=stats_columns, \n",
    "    )\n",
    "    \n",
    "    all_phase_flight_stats = []\n",
    "    all_phases_dict = generate_phase_segments(flight_df)\n",
    "    for phase in possible_phases:\n",
    "        phase_df = flight_df.loc[flight_df.phase==phase]\n",
    "        if len(phase_df):\n",
    "            # global_phase_variability_stats\n",
    "            phase_flight_stats = get_flight_stats_row(\n",
    "                flight_df=phase_df,\n",
    "                n_quantiles=n_quantiles,\n",
    "                stats_columns=stats_columns,\n",
    "            )\n",
    "            n_phase_segments = len(all_phases_dict[phase])\n",
    "            phase_flight_stats = pd.concat([\n",
    "                pd.Series(\n",
    "                    data=[n_phase_segments],\n",
    "                    index=[\"n_segments\"],\n",
    "                ),\n",
    "                phase_flight_stats\n",
    "            ])\n",
    "            # inter_segment_phase_variability_stats\n",
    "            # how much change is there between segments in the same phase\n",
    "            if n_phase_segments > 1:\n",
    "                stdvs_segment_coefficients_of_variation = []\n",
    "                means_segment_coefficients_of_variation = []\n",
    "                for phase_segment_idxs in all_phases_dict[phase]:\n",
    "                    segment = phase_df.loc[phase_segment_idxs]\n",
    "                    segment_coefficients_of_variation = (\n",
    "                        segment[stats_columns].std(ddof=0) / abs(segment[stats_columns].mean())\n",
    "                    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "                    stdv_segment_coefficients_of_variation = segment_coefficients_of_variation.std(ddof=0)\n",
    "                    mean_segment_coefficients_of_variation = segment_coefficients_of_variation.mean()\n",
    "                    stdvs_segment_coefficients_of_variation.append(stdv_segment_coefficients_of_variation)\n",
    "                    means_segment_coefficients_of_variation.append(mean_segment_coefficients_of_variation)\n",
    "                \n",
    "                stdv_stdvs_segment_coefficients_of_variation = np.std(stdvs_segment_coefficients_of_variation, ddof=0)\n",
    "                stdv_means_segment_coefficients_of_variation = np.std(means_segment_coefficients_of_variation, ddof=0)\n",
    "                inter_segment_variability_stats = pd.Series(\n",
    "                    data=[\n",
    "                        stdv_stdvs_segment_coefficients_of_variation, \n",
    "                        stdv_means_segment_coefficients_of_variation\n",
    "                    ],\n",
    "                    index=[\n",
    "                        f\"{phase}_stdv_stdvs_segment_coefficients_of_variation\", \n",
    "                        f\"{phase}_stdv_means_segment_coefficients_of_variation\", \n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                inter_segment_variability_stats = pd.Series(\n",
    "                    data=[\n",
    "                        0, \n",
    "                        0\n",
    "                    ],\n",
    "                    index=[\n",
    "                        f\"{phase}_stdv_stdvs_segment_coefficients_of_variation\", \n",
    "                        f\"{phase}_stdv_means_segment_coefficients_of_variation\", \n",
    "                    ]\n",
    "                )\n",
    "        else:\n",
    "            phase_flight_stats = pd.Series(\n",
    "                np.repeat(np.nan, len(global_flight_stats)),\n",
    "                index=global_flight_stats.index\n",
    "            )\n",
    "            phase_flight_stats = pd.concat([\n",
    "                pd.Series(\n",
    "                    data=[0],\n",
    "                    index=[\"n_segments\"],\n",
    "                ),\n",
    "                phase_flight_stats\n",
    "            ])\n",
    "            inter_segment_variability_stats = pd.Series(\n",
    "                data=[\n",
    "                    np.nan, \n",
    "                    np.nan,\n",
    "                ],\n",
    "                index=[\n",
    "                    f\"{phase}_stdv_stdvs_segment_coefficients_of_variation\", \n",
    "                    f\"{phase}_stdv_means_segment_coefficients_of_variation\", \n",
    "                ]\n",
    "            )\n",
    "        phase_flight_stats.index = [f\"{phase}_{index}\" for index in phase_flight_stats.index]\n",
    "        all_phase_flight_stats.append(\n",
    "            pd.concat([\n",
    "                inter_segment_variability_stats,\n",
    "                phase_flight_stats\n",
    "            ])\n",
    "        )\n",
    "    all_phase_flight_stats = pd.concat(all_phase_flight_stats)\n",
    "    flight_stats = pd.concat([global_flight_stats, all_phase_flight_stats])\n",
    "    flight_stats_df = pd.DataFrame(flight_stats, columns=[int(flight_df.flight_id.unique())]).T\n",
    "    return flight_stats_df\n",
    "\n",
    "\n",
    "def process_flight_stats_batch(flight_ids_batch, flights_folder, n_quantiles, stats_columns, possible_phases):\n",
    "    \"\"\"\n",
    "    Processes a batch of flight IDs and computes statistics for each flight.\n",
    "\n",
    "    Parameters:\n",
    "    flight_ids_batch (list): List of flight IDs to process.\n",
    "    flights_folder (str): Path to the folder containing flight data.\n",
    "    n_quantiles (int): Number of quantiles to compute.\n",
    "    stats_columns (list): List of column names to compute statistics on.\n",
    "    possible_phases (list): List of possible flight phases to consider.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing statistics for all flights in the batch.\n",
    "    \"\"\"\n",
    "    flight_stats_df_list = []\n",
    "    for flight_id in flight_ids_batch:\n",
    "        flight_df = joblib.load(os.path.join(flights_folder, str(flight_id)))\n",
    "        flight_stats_df = get_flight_stats_df(\n",
    "            flight_df=flight_df, \n",
    "            n_quantiles=n_quantiles,\n",
    "            stats_columns=stats_columns, \n",
    "            possible_phases=possible_phases,\n",
    "        )\n",
    "        flight_stats_df_list.append(flight_stats_df)\n",
    "    flight_stats_df = pd.concat(flight_stats_df_list, axis=0)\n",
    "    return flight_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wingspan</th>\n",
       "      <th>Wingspan_dif_winglets</th>\n",
       "      <th>Num_Engines</th>\n",
       "      <th>Approach_Speed_knot</th>\n",
       "      <th>Length_ft</th>\n",
       "      <th>Tail_Height_at_OEW_ft</th>\n",
       "      <th>Wheelbase_ft</th>\n",
       "      <th>Cockpit_to_Main_Gear_ft</th>\n",
       "      <th>Main_Gear_Width_ft</th>\n",
       "      <th>MTOW_lb</th>\n",
       "      <th>MALW_lb</th>\n",
       "      <th>Parking_Area_ft2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICAO_Code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A20N</th>\n",
       "      <td>117.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>137.0</td>\n",
       "      <td>123.3</td>\n",
       "      <td>39.6</td>\n",
       "      <td>41.5</td>\n",
       "      <td>50.2</td>\n",
       "      <td>29.4</td>\n",
       "      <td>174165.0</td>\n",
       "      <td>148591.0</td>\n",
       "      <td>16358.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A21N</th>\n",
       "      <td>117.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>136.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>55.5</td>\n",
       "      <td>64.2</td>\n",
       "      <td>29.4</td>\n",
       "      <td>209439.0</td>\n",
       "      <td>174606.0</td>\n",
       "      <td>19253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A310</th>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>139.0</td>\n",
       "      <td>153.2</td>\n",
       "      <td>52.3</td>\n",
       "      <td>49.9</td>\n",
       "      <td>63.9</td>\n",
       "      <td>36.0</td>\n",
       "      <td>317465.0</td>\n",
       "      <td>273372.0</td>\n",
       "      <td>24361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A319</th>\n",
       "      <td>114.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2</td>\n",
       "      <td>126.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>44.9</td>\n",
       "      <td>29.4</td>\n",
       "      <td>168653.0</td>\n",
       "      <td>134482.0</td>\n",
       "      <td>14140.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A320</th>\n",
       "      <td>114.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2</td>\n",
       "      <td>136.0</td>\n",
       "      <td>123.3</td>\n",
       "      <td>39.6</td>\n",
       "      <td>41.5</td>\n",
       "      <td>50.2</td>\n",
       "      <td>29.4</td>\n",
       "      <td>171961.0</td>\n",
       "      <td>145505.0</td>\n",
       "      <td>16354.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Wingspan  Wingspan_dif_winglets  Num_Engines  Approach_Speed_knot  \\\n",
       "ICAO_Code                                                                      \n",
       "A20N          117.5                    0.0            2                137.0   \n",
       "A21N          117.5                    0.0            2                136.0   \n",
       "A310          144.0                    0.0            2                139.0   \n",
       "A319          114.7                    5.6            2                126.0   \n",
       "A320          114.7                    5.6            2                136.0   \n",
       "\n",
       "           Length_ft  Tail_Height_at_OEW_ft  Wheelbase_ft  \\\n",
       "ICAO_Code                                                   \n",
       "A20N           123.3                   39.6          41.5   \n",
       "A21N           146.0                   39.7          55.5   \n",
       "A310           153.2                   52.3          49.9   \n",
       "A319           111.0                   39.7          36.2   \n",
       "A320           123.3                   39.6          41.5   \n",
       "\n",
       "           Cockpit_to_Main_Gear_ft  Main_Gear_Width_ft   MTOW_lb   MALW_lb  \\\n",
       "ICAO_Code                                                                    \n",
       "A20N                          50.2                29.4  174165.0  148591.0   \n",
       "A21N                          64.2                29.4  209439.0  174606.0   \n",
       "A310                          63.9                36.0  317465.0  273372.0   \n",
       "A319                          44.9                29.4  168653.0  134482.0   \n",
       "A320                          50.2                29.4  171961.0  145505.0   \n",
       "\n",
       "           Parking_Area_ft2  \n",
       "ICAO_Code                    \n",
       "A20N                16358.3  \n",
       "A21N                19253.0  \n",
       "A310                24361.0  \n",
       "A319                14140.5  \n",
       "A320                16354.4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load aircraft data from an Excel file and filter rows to include only aircraft types found in `cha_df`\n",
    "aircraft_df = pd.read_excel(\"aircraft.xlsx\")\n",
    "aircraft_df = aircraft_df.loc[\n",
    "    aircraft_df.ICAO_Code.isin(cha_df[\"aircraft_type\"].unique())\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Calculate the average wingspan by averaging values with and without winglets/sharklets\n",
    "aircraft_df[\"Wingspan\"] = pd.DataFrame(\n",
    "    list(zip(\n",
    "        aircraft_df.Wingspan_ft_without_winglets_sharklets, \n",
    "        aircraft_df.Wingspan_ft_with_winglets_sharklets\n",
    "    ))\n",
    ").T.mean().values\n",
    "\n",
    "# Calculate the difference in wingspan with and without winglets/sharklets\n",
    "aircraft_df[\"Wingspan_dif_winglets\"] = pd.DataFrame(\n",
    "    list(zip(\n",
    "        aircraft_df.Wingspan_ft_without_winglets_sharklets, \n",
    "        aircraft_df.Wingspan_ft_with_winglets_sharklets\n",
    "    ))\n",
    ").T.diff().dropna(how=\"all\").fillna(0).values[0]\n",
    "\n",
    "# Select relevant columns for analysis and set 'ICAO_Code' as the DataFrame index\n",
    "aircraft_df = aircraft_df[\n",
    "    [\n",
    "        \"ICAO_Code\",\n",
    "        \"Wingspan\",\n",
    "        \"Wingspan_dif_winglets\",\n",
    "        \"Num_Engines\",\n",
    "        \"Approach_Speed_knot\",\n",
    "        \"Length_ft\",\n",
    "        \"Tail_Height_at_OEW_ft\",\n",
    "        \"Wheelbase_ft\",\n",
    "        \"Cockpit_to_Main_Gear_ft\",\n",
    "        \"Main_Gear_Width_ft\",\n",
    "        \"MTOW_lb\",\n",
    "        \"MALW_lb\", \n",
    "        \"Parking_Area_ft2\",\n",
    "    ]\n",
    "]\n",
    "aircraft_df.index = aircraft_df.ICAO_Code\n",
    "aircraft_df = aircraft_df.drop(columns=[\"ICAO_Code\"])\n",
    "\n",
    "# Display the first few rows of the modified aircraft DataFrame\n",
    "aircraft_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEWLY ADDED BY BERNARD - NO OTHER CHANGES\n",
    "\n",
    "data_folder = \"./../../data\"\n",
    "\n",
    "print(aircraft_df.shape) # 12 features\n",
    "\n",
    "# Read engine data\n",
    "engine_info_loc = os.path.join(data_folder, \"ICAO_engine_data.xlsx\")\n",
    "engine_df = pd.read_excel(engine_info_loc)\n",
    "engine_df.index = engine_df.ICAO_Code\n",
    "engine_df = engine_df.drop(columns=[\"ICAO_Code\"])\n",
    "# Add engine data\n",
    "aircraft_df = aircraft_df.join(engine_df) # Adds 80 features\n",
    "\n",
    "# Read some additional features for the acropole fuel estimation\n",
    "acropole_features_loc = os.path.join(data_folder, \"aircraft_params_acropole_BB.csv\")\n",
    "acropole_features_df = pd.read_csv(acropole_features_loc)\n",
    "acropole_features_df = acropole_features_df.rename(columns={\"ACFT_ICAO_TYPE\": \"ICAO_Code\"})\n",
    "acropole_features_df.index = acropole_features_df.ICAO_Code\n",
    "acropole_features_df = acropole_features_df.drop(columns=[\"ICAO_Code\"])\n",
    "\n",
    "# Add acropole features\n",
    "aircraft_df = aircraft_df.join(acropole_features_df, how='left', lsuffix='_left') # Adds 17 features\n",
    "\n",
    "print(aircraft_df.shape) # Now 109 features\n",
    "aircraft_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>nRunways</th>\n",
       "      <th>nIn</th>\n",
       "      <th>nOut</th>\n",
       "      <th>inDegree</th>\n",
       "      <th>outDegree</th>\n",
       "      <th>pageRank</th>\n",
       "      <th>load</th>\n",
       "      <th>betweenness</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>percolation</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>closeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EBAW</th>\n",
       "      <td>51.190667</td>\n",
       "      <td>4.463153</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>507</td>\n",
       "      <td>508</td>\n",
       "      <td>0.022917</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>2.412563e-03</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.933619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTC</th>\n",
       "      <td>69.683296</td>\n",
       "      <td>18.918900</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>267</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.158608e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3568.370084</td>\n",
       "      <td>3.038518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LKPR</th>\n",
       "      <td>50.100874</td>\n",
       "      <td>14.259911</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1651</td>\n",
       "      <td>1771</td>\n",
       "      <td>0.022917</td>\n",
       "      <td>0.022917</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>5.371437e-02</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>25634.975158</td>\n",
       "      <td>4.025762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTAJ</th>\n",
       "      <td>36.947201</td>\n",
       "      <td>37.478699</td>\n",
       "      <td>2315.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.466238e-69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4490.509100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OIIE</th>\n",
       "      <td>35.416100</td>\n",
       "      <td>51.152199</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.466238e-69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17212.946940</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       latitude  longitude  elevation  nRunways   nIn  nOut  inDegree  \\\n",
       "EBAW  51.190667   4.463153       39.0       1.0   507   508  0.022917   \n",
       "ENTC  69.683296  18.918900       31.0       1.0    21   267  0.004167   \n",
       "LKPR  50.100874  14.259911     1247.0       3.0  1651  1771  0.022917   \n",
       "LTAJ  36.947201  37.478699     2315.0       2.0     0    69  0.000000   \n",
       "OIIE  35.416100  51.152199     3305.0       2.0     0    20  0.000000   \n",
       "\n",
       "      outDegree  pageRank      load  betweenness   eigenvector  percolation  \\\n",
       "EBAW   0.018750  0.002071  0.001983     0.001983  2.412563e-03     0.001983   \n",
       "ENTC   0.008333  0.000367  0.000000     0.000000  2.158608e-04     0.000000   \n",
       "LKPR   0.022917  0.003668  0.004515     0.004653  5.371437e-02     0.004653   \n",
       "LTAJ   0.010417  0.000326  0.000000     0.000000  5.466238e-69     0.000000   \n",
       "OIIE   0.002083  0.000326  0.000000     0.000000  5.466238e-69     0.000000   \n",
       "\n",
       "          harmonic  closeness  \n",
       "EBAW      0.000000   3.933619  \n",
       "ENTC   3568.370084   3.038518  \n",
       "LKPR  25634.975158   4.025762  \n",
       "LTAJ   4490.509100   0.000000  \n",
       "OIIE  17212.946940   0.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating features for each airport\n",
    "airport_df = pd.DataFrame(index=list(set(cha_df.adep.unique()).union(set(cha_df.ades.unique()))))\n",
    "\n",
    "# lat lon and elevation\n",
    "lat_lon_el = airports_df.loc[\n",
    "    airports_df.ident.isin(airport_df.index), \n",
    "    [\"latitude_deg\", \"longitude_deg\", \"elevation_ft\",]\n",
    "].values\n",
    "airport_order = airports_df.loc[\n",
    "    airports_df.ident.isin(airport_df.index)\n",
    "].ident.values\n",
    "airport_df.loc[airport_order, \"latitude\"] = lat_lon_el[:,0]\n",
    "airport_df.loc[airport_order, \"longitude\"] = lat_lon_el[:,1]\n",
    "airport_df.loc[airport_order, \"elevation\"] = lat_lon_el[:,2]\n",
    "\n",
    "# number of runways\n",
    "n_runways = runway_df.loc[\n",
    "    runway_df.airport_ident.isin(airport_df.index), \n",
    "    \"airport_ident\"\n",
    "].value_counts().to_dict()\n",
    "airport_df.loc[n_runways.keys(), \"nRunways\"] = list(n_runways.values())\n",
    "\n",
    "# number of incoming and outgoing flights\n",
    "G = nx.DiGraph()\n",
    "airport_df[\"nIn\"] = 0\n",
    "airport_df[\"nOut\"] = 0\n",
    "dep_des_ws = cha_df[[\"adep\", \"ades\"]].value_counts().to_dict()\n",
    "for dep_des_w in dep_des_ws:\n",
    "    (u, v), w = dep_des_w, dep_des_ws[dep_des_w]\n",
    "    G.add_edge(u, v, weight=w, weight_inv=1/w)\n",
    "    airport_df.loc[u, \"nOut\"] += w\n",
    "    airport_df.loc[v, \"nIn\"] += w\n",
    "    \n",
    "components = sorted(\n",
    "    [component for component in nx.components.weakly_connected_components(G)],\n",
    "    key = lambda comp: len(comp)\n",
    ")[::-1]\n",
    "lwc_component = components[0]\n",
    "G = G.subgraph(lwc_component)\n",
    "\n",
    "# centrality measures\n",
    "airport_df.loc[list(G.nodes), \"inDegree\"] = list(nx.in_degree_centrality(G).values())\n",
    "airport_df.loc[list(G.nodes), \"outDegree\"] = list(nx.out_degree_centrality(G).values())\n",
    "airport_df.loc[list(G.nodes), \"pageRank\"] = list(nx.pagerank(G, weight=\"weight\").values())\n",
    "airport_df.loc[list(G.nodes), \"load\"] = list(nx.load_centrality(G, weight=\"weight\").values())\n",
    "airport_df.loc[list(G.nodes), \"betweenness\"] = list(nx.betweenness_centrality(G, weight=\"weight\").values())\n",
    "airport_df.loc[list(G.nodes), \"eigenvector\"] = list(nx.eigenvector_centrality(G, weight=\"weight\").values())\n",
    "airport_df.loc[list(G.nodes), \"percolation\"] = list(nx.percolation_centrality(G, weight=\"weight\").values())\n",
    "airport_df.loc[list(G.nodes), \"harmonic\"] = list(nx.harmonic_centrality(G, distance=\"weight_inv\").values())\n",
    "airport_df.loc[list(G.nodes), \"closeness\"] = list(nx.closeness_centrality(G, distance=\"weight_inv\").values())\n",
    "\n",
    "airport_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c9bb5b69b54532b1b745b5bcf2e75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464019c3c092436fb2c65a3362a0a345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the flight phases to consider and exclude NA as it introduces noise\n",
    "possible_phases = [\"CLIMB\", \"CRUISE\", \"DESCENT\", \"GROUND\", \"LEVEL\"]\n",
    "\n",
    "# Specify the flight features (stats) of interest for processing\n",
    "stats_columns = [\n",
    "    \"wind_x\", \"wind_y\", \"track_x\", \"track_y\", \"heading_x\", \"heading_y\",\n",
    "    \"temperature\", \"specific_humidity\", \"latitude\", \"longitude\", \"altitude\", \n",
    "    \"groundspeed\", \"TAS\", \"TdG_speed\", \"ToG_speed\", \"vertical_rate\", \n",
    "    \"wind_mag\", \"track_wind_dot\", \"track_heading_dot\", \"heading_wind_dot\",\n",
    "]\n",
    "\n",
    "# Create a list of all flight IDs from files in the flights folder\n",
    "flight_ids = [int(file) for file in os.listdir(flights_folder) if \".\" not in file]\n",
    "\n",
    "# Split the flight IDs into batches for parallel processing\n",
    "num_batches = 10000\n",
    "flight_ids_batches = [\n",
    "    flight_ids[i*(len(flight_ids)//num_batches):(i+1)*(len(flight_ids)//num_batches)]\n",
    "    for i in range(num_batches)\n",
    "]\n",
    "# Distribute any remaining flight IDs across the batches\n",
    "if len(flight_ids) % num_batches:\n",
    "    missing_flight_ids = flight_ids[num_batches*(len(flight_ids)//num_batches):]\n",
    "    for i in range(len(missing_flight_ids)):\n",
    "        flight_ids_batches[i] += [missing_flight_ids[i]]\n",
    "\n",
    "# Process flight statistics for different quantile levels if data is not already saved\n",
    "for n_quantiles in [3, 5, 11]:\n",
    "    if f\"flight_stats_df_cha_nq={n_quantiles}.pkl\" not in os.listdir(os.getcwd()):\n",
    "        # Use parallel processing to calculate flight statistics in batches\n",
    "        all_flight_stats_df_list = Parallel(n_jobs=12)(\n",
    "            delayed(process_flight_stats_batch)(\n",
    "                n_quantiles=n_quantiles, \n",
    "                stats_columns=stats_columns,\n",
    "                flights_folder=flights_folder,\n",
    "                possible_phases=possible_phases,\n",
    "                flight_ids_batch=flight_ids_batch,\n",
    "            )\n",
    "            for flight_ids_batch in tqdm(flight_ids_batches)\n",
    "        )\n",
    "        # Concatenate all results and save to a file\n",
    "        flight_stats_df = pd.concat(all_flight_stats_df_list, axis=0)\n",
    "        joblib.dump(flight_stats_df, f\"flight_stats_df_cha_nq={n_quantiles}.pkl\")\n",
    "    else:\n",
    "        # Load existing processed flight statistics data\n",
    "        flight_stats_df = joblib.load(f\"flight_stats_df_cha_nq={n_quantiles}.pkl\")\n",
    "        \n",
    "    # Create a copy of `cha_df` and add cyclic features for arrival and departure times\n",
    "    X_y_df = cha_df.copy()\n",
    "    X_y_df.arrival_time = pd.to_datetime(X_y_df.arrival_time)\n",
    "    X_y_df.actual_offblock_time = pd.to_datetime(X_y_df.actual_offblock_time)\n",
    "    \n",
    "    # Add cyclic time features (day, week, month, year) for arrival and departure times\n",
    "    X_y_df[\"arrDayX\"], X_y_df[\"arrDayY\"] = day_cyclic_values(X_y_df.arrival_time)\n",
    "    X_y_df[\"arrWeekX\"], X_y_df[\"arrWeekY\"] = week_cyclic_values(X_y_df.arrival_time)\n",
    "    X_y_df[\"arrMonthX\"], X_y_df[\"arrMonthY\"] = month_cyclic_values(X_y_df.arrival_time)\n",
    "    X_y_df[\"arrYearX\"], X_y_df[\"arrYearY\"] = year_cyclic_values(X_y_df.arrival_time)\n",
    "    \n",
    "    X_y_df[\"depDayX\"], X_y_df[\"depDayY\"] = day_cyclic_values(X_y_df.actual_offblock_time)\n",
    "    X_y_df[\"depWeekX\"], X_y_df[\"depWeekY\"] = week_cyclic_values(X_y_df.actual_offblock_time)\n",
    "    X_y_df[\"depMonthX\"], X_y_df[\"depMonthY\"] = month_cyclic_values(X_y_df.actual_offblock_time)\n",
    "    X_y_df[\"depYearX\"], X_y_df[\"depYearY\"] = year_cyclic_values(X_y_df.actual_offblock_time)\n",
    "    \n",
    "    # Merge `aircraft_df` and `airport_df` to add additional aircraft and airport features\n",
    "    X_y_df = X_y_df.merge(right=aircraft_df, how=\"left\", left_on=\"aircraft_type\", right_index=True)\n",
    "    X_y_df = X_y_df.merge(right=airport_df, how=\"left\", left_on=\"adep\", right_index=True)\n",
    "    X_y_df = X_y_df.merge(right=airport_df, how=\"left\", left_on=\"ades\", right_index=True, suffixes=(\"_adep\", \"_ades\"))\n",
    "    \n",
    "    # Consolidate less common airlines into an \"other\" category and one-hot encode airlines\n",
    "    airline_value_counts = X_y_df.airline.value_counts(normalize=True)\n",
    "    top_airlines_n = np.argmax(airline_value_counts.cumsum().diff() < 1e-2)\n",
    "    arlines_to_discard = airline_value_counts.keys()[top_airlines_n:]\n",
    "    X_y_df.loc[X_y_df.airline.isin(arlines_to_discard).values.astype(bool), \"airline\"] = \"other\"\n",
    "    airline_dummie_df  = pd.get_dummies(X_y_df.airline).astype(float)\n",
    "    airline_dummie_df.columns = [\"airline_\" + column for column in airline_dummie_df.columns]\n",
    "    X_y_df = pd.concat([X_y_df, airline_dummie_df], axis=1)\n",
    "    \n",
    "    # Drop columns that are not needed for analysis\n",
    "    X_y_df = X_y_df.drop(columns=[\n",
    "        \"callsign\", \"wtc\", \"airline\", \n",
    "        \"name_adep\", \"country_code_adep\", \n",
    "        \"name_ades\", \"country_code_ades\",\n",
    "        \"actual_offblock_time\", \"arrival_time\",\n",
    "        \"aircraft_type\", \"adep\", \"ades\"\n",
    "    ])\n",
    "    \n",
    "    # Calculate new airport features based on the difference between departure and arrival attributes\n",
    "    adep_columns = [column for column in X_y_df.columns if \"_adep\" in column]\n",
    "    ades_columns = [column for column in X_y_df.columns if \"_ades\" in column]\n",
    "    for i in range(len(adep_columns)):\n",
    "        adep_column = adep_columns[i]\n",
    "        ades_column = ades_columns[i]\n",
    "        diff_col_name = f\"{adep_column}_diff\"\n",
    "        X_y_df[diff_col_name] = X_y_df[ades_column] - X_y_df[adep_column]    \n",
    "    X_y_df[\"airports_distance\"] = calculate_distances(X_y_df[[\"latitude_adep\", \"longitude_adep\", \"latitude_ades\", \"longitude_ades\"]])\n",
    "    \n",
    "    # Merge with flight statistics DataFrame to add calculated flight features\n",
    "    X_y_df = X_y_df.merge(right=flight_stats_df, how=\"left\", left_on=\"flight_id\", right_index=True)\n",
    "    \n",
    "    # Adjust column order and move `tow` to the end\n",
    "    column_order = X_y_df.columns.tolist()\n",
    "    column_order.remove(\"tow\")\n",
    "    column_order += [\"tow\"]\n",
    "    X_y_df = X_y_df[column_order]\n",
    "\n",
    "    # Fill any remaining NaN values with a placeholder outside the range of existing values\n",
    "    feature_columns = X_y_df.columns[2:-1]\n",
    "    for feature_column in feature_columns:\n",
    "        if X_y_df[feature_column].isna().sum():\n",
    "            min_value = X_y_df[feature_column].min()\n",
    "            max_value = X_y_df[feature_column].max()\n",
    "            feature_range = max_value - min_value\n",
    "            nan_placeholder = min_value - feature_range\n",
    "            X_y_df[feature_column] = X_y_df[feature_column].fillna(nan_placeholder)\n",
    "    \n",
    "    # Save the final DataFrame with flight features and labels for further analysis\n",
    "    joblib.dump(X_y_df, f\"X_y_df_cha_nq={n_quantiles}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0245bcebe6c946a7912f6ff91e35a182": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a813428e3191444b89170b45caaf3e4a",
       "style": "IPY_MODEL_8d7f9a5bfdf74109bbc84db88b14b3e0",
       "value": " 10000/10000 [1:56:31&lt;00:00,  1.28it/s]"
      }
     },
     "0499945eb76f4aa7b8dfced8c6a8aed9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bffbeb6e4f8a49e1950f53b114b9a674",
       "style": "IPY_MODEL_38872cacc29e4ef0b909ddcc9db67ec9",
       "value": "100%"
      }
     },
     "1b72f93b53ba458b9ad2844880e82742": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30fccb2120fb416dbb5c2ac030ddca0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9d4522b787744d2799995ed7e90726e0",
       "style": "IPY_MODEL_48ce27822d0547f386304b4bbc5827dd",
       "value": " 10000/10000 [2:58:45&lt;00:00,  1.02s/it]"
      }
     },
     "38872cacc29e4ef0b909ddcc9db67ec9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "41279433165a453da766273122afa211": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_9ff172d569eb437d8ce65ffff511a1fe",
       "max": 10000,
       "style": "IPY_MODEL_beaa2bfd74ae43cfb253dbe51f977f18",
       "value": 10000
      }
     },
     "464019c3c092436fb2c65a3362a0a345": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_498f15ba609a4cdd8b34ee789f2dfa14",
        "IPY_MODEL_41279433165a453da766273122afa211",
        "IPY_MODEL_30fccb2120fb416dbb5c2ac030ddca0c"
       ],
       "layout": "IPY_MODEL_ac1e684c59574da79dfcf86108319448"
      }
     },
     "48ce27822d0547f386304b4bbc5827dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "498f15ba609a4cdd8b34ee789f2dfa14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a27da015f8674400b6cc89786f662d62",
       "style": "IPY_MODEL_6b1728c852a941709f112b5c57ae5a9c",
       "value": "100%"
      }
     },
     "592203dbe4d643ee8e51715bbd104497": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_1b72f93b53ba458b9ad2844880e82742",
       "max": 10000,
       "style": "IPY_MODEL_bcf8b1bde00d4b9cb4dc8dd62b21d663",
       "value": 10000
      }
     },
     "6b1728c852a941709f112b5c57ae5a9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8d7f9a5bfdf74109bbc84db88b14b3e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d4522b787744d2799995ed7e90726e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9ff172d569eb437d8ce65ffff511a1fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a27da015f8674400b6cc89786f662d62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a5e54a642f5b420290aaf310d5c6a547": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a813428e3191444b89170b45caaf3e4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ac1e684c59574da79dfcf86108319448": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bcf8b1bde00d4b9cb4dc8dd62b21d663": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "beaa2bfd74ae43cfb253dbe51f977f18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "bffbeb6e4f8a49e1950f53b114b9a674": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e2c9bb5b69b54532b1b745b5bcf2e75a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0499945eb76f4aa7b8dfced8c6a8aed9",
        "IPY_MODEL_592203dbe4d643ee8e51715bbd104497",
        "IPY_MODEL_0245bcebe6c946a7912f6ff91e35a182"
       ],
       "layout": "IPY_MODEL_a5e54a642f5b420290aaf310d5c6a547"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
