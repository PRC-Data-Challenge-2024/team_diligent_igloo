{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "<small>This notebook includes functions to preprocess and analyze flight data by resampling, calculating distances and bearings, and identifying the appropriate flight phases. Key functions include:\n",
    "\n",
    "- normalise_time_df: Resamples a DataFrame to a specified frequency (time_freq), such as 1-second intervals, interpolating missing values in selected columns (cols_to_interpol) with the specified interpolation method (e.g., akima). Static columns are forward-filled.\n",
    "\n",
    "- haversine_np: Calculates the great-circle distance between two points (given latitudes and longitudes) using the Haversine formula in meters.\n",
    "\n",
    "- calculate_distances: Computes distances between consecutive latitude and longitude points in a DataFrame.\n",
    "\n",
    "- calculate_bearing and calculate_bearings: Calculate the initial bearing (or direction) between two geographical points, with calculate_bearings applying this over a DataFrame.\n",
    "\n",
    "- generate_phase_segments: Detects segments in a flight phase DataFrame where the phase changes, storing each segment's indices in a dictionary for easy access. We also improve on the fuzzy approach used in the Traffic by fixing some parts that are not identified properly. \n",
    "\n",
    "- preprocess_flight: Processes a flight by applying all the preprocessing functions, creating a resampled, clean flight.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'traffic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d0dcfedae9ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtraffic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtraffic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTraffic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'traffic'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from traffic.core import Flight\n",
    "from traffic.core import Traffic\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the files \n",
    "data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "flights_folder = os.path.join(os.getcwd(), \"flightDfs\")\n",
    "cha_df = pd.read_csv(os.path.join(data_folder, \"challenge_set.csv\"))\n",
    "sub_df = pd.read_csv(os.path.join(data_folder, \"submission_set.csv\"))\n",
    "final_sub_df = pd.read_csv(os.path.join(data_folder, \"final_submission_set.csv\"))\n",
    "\n",
    "#get the paths from the parquet files\n",
    "parquet_paths = sorted([\n",
    "    os.path.join(data_folder, file) for file in os.listdir(data_folder) if \".parquet\" in file\n",
    "])\n",
    "\n",
    "#identify the flights we need\n",
    "usable_flight_ids = list(set(cha_df.flight_id.unique()).union(\n",
    "    set(sub_df.flight_id.unique()).union(\n",
    "        set(final_sub_df.flight_id.unique())\n",
    "    )\n",
    "))\n",
    "#remove unnecessary files\n",
    "del cha_df, sub_df, final_sub_df\n",
    "\n",
    "#make the folder if it isn't there\n",
    "if not os.path.isdir(flights_folder):\n",
    "    os.mkdir(flights_folder)\n",
    "\n",
    "# Collect a list of complete flights by checking existing files in the folder\n",
    "# (file names that do not contain a dot are considered valid flight data files)\n",
    "complete_flights=[\n",
    "    file for file in os.listdir(flights_folder) if \".\" not in file\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_time_df(df, time_freq=\"1s\", interpol_method=\"akima\", time_column=\"timestamp\", cols_to_interpol=[]):\n",
    "    group_df = df.copy()\n",
    "    group_df[time_column] = pd.to_datetime(group_df[time_column])  # Ensure time_column is a datetime column\n",
    "    group_df = group_df.sort_values(by=\"timestamp\")\n",
    "\n",
    "    static_columns = group_df.columns.tolist()\n",
    "    static_columns.remove(time_column)\n",
    "    [static_columns.remove(interpol_column) for interpol_column in cols_to_interpol]\n",
    "    \n",
    "    # Generate a new DataFrame with a continuous 1-second interval for timestamps\n",
    "    start_time = group_df[time_column].min()\n",
    "    end_time = group_df[time_column].max()\n",
    "    \n",
    "    # Create a new range of timestamps at 1 second intervals\n",
    "    new_timestamps = pd.date_range(start=start_time, end=end_time, freq=time_freq)\n",
    "    \n",
    "    # Reindex the dataframe with the new timestamps\n",
    "    group_df.set_index(time_column, inplace=True)  # Set time_column as index for reindexing\n",
    "    group_df = group_df.reindex(new_timestamps)  # Reindex to include every second\n",
    "    \n",
    "    # Interpolating columns\n",
    "    group_df[cols_to_interpol] = group_df[cols_to_interpol].interpolate(method=interpol_method)\n",
    "\n",
    "    # Forward fill constant values\n",
    "    group_df[static_columns] = group_df[static_columns].ffill()\n",
    "    \n",
    "    # Reset the index to make time_column a column again\n",
    "    group_df.reset_index(inplace=True)\n",
    "    group_df = group_df.rename(columns={'index': time_column})\n",
    "\n",
    "    return group_df\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Compute the great-circle distance between two points on the Earth's surface using NumPy, in meters.\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Radius of earth in meters (6371 km converted to meters)\n",
    "    r = 6371000\n",
    "    return c * r\n",
    "\n",
    "def calculate_distances(df):\n",
    "    # Shift the lat/lon columns to create the \"previous\" row\n",
    "    lat1 = df['latitude'][:-1].values\n",
    "    lon1 = df['longitude'][:-1].values\n",
    "    lat2 = df['latitude'][1:].values\n",
    "    lon2 = df['longitude'][1:].values\n",
    "    \n",
    "    # Compute the distance between consecutive points\n",
    "    distances = haversine_np(lon1, lat1, lon2, lat2)\n",
    "    \n",
    "    # Append a NaN for the first distance (as there's no previous point for the first row)\n",
    "    distances = np.insert(distances, 0, np.nan)\n",
    "    \n",
    "    # Add distances to DataFrame in meters\n",
    "    return distances\n",
    "\n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the bearing (track angle) between two points using latitude and longitude.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "    \n",
    "    # Calculate the change in longitude\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Calculate bearing using the formula\n",
    "    x = np.sin(dlon) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "    \n",
    "    initial_bearing = np.arctan2(x, y)\n",
    "    \n",
    "    # Convert from radians to degrees\n",
    "    initial_bearing = np.degrees(initial_bearing)\n",
    "    \n",
    "    # Normalize the bearing to 0 - 360 degrees\n",
    "    bearing = (initial_bearing + 360) % 360\n",
    "    \n",
    "    return bearing\n",
    "\n",
    "def calculate_bearings(df):\n",
    "    # Shift the lat/lon columns to create the \"previous\" row\n",
    "    lat1 = df['latitude'][:-1].values\n",
    "    lon1 = df['longitude'][:-1].values\n",
    "    lat2 = df['latitude'][1:].values\n",
    "    lon2 = df['longitude'][1:].values\n",
    "    \n",
    "    # Compute the bearing between consecutive points\n",
    "    bearings = calculate_bearing(lat1, lon1, lat2, lon2)\n",
    "    \n",
    "    # Append a NaN for the first row as there's no previous point for it\n",
    "    bearings = np.insert(bearings, 0, np.nan)\n",
    "    \n",
    "    return bearings\n",
    "\n",
    "def generate_phase_segments(df):\n",
    "    df = df[[\"phase\"]].copy()\n",
    "    # Create a shifted phase column to detect phase changes\n",
    "    df['shifted_phase'] = df['phase'].shift(1)\n",
    "\n",
    "    # Mark where phase changes (start of a new segment)\n",
    "    df['segment'] = (df['phase'] != df['shifted_phase']).cumsum()\n",
    "\n",
    "    # Initialize the dictionary to store the segments\n",
    "    phase_segments = {}\n",
    "\n",
    "    # Use groupby to group by phase and segment and collect indices\n",
    "    for (phase, segment), group in df.groupby(['phase', 'segment']):\n",
    "        if phase not in phase_segments:\n",
    "            phase_segments[phase] = []\n",
    "        phase_segments[phase].append(np.array(list(group.index)))\n",
    "\n",
    "    return phase_segments\n",
    "\n",
    "def preprocess_flight(flight, output_folder=\"flights_folder\"):\n",
    "    flight.data = flight.data.drop(columns=[\"icao24\"]).rename(\n",
    "        columns={\n",
    "            \"u_component_of_wind\": \"wind_u\",\n",
    "            \"v_component_of_wind\": \"wind_v\"\n",
    "        }\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    flight = flight.filter(\n",
    "        filter=\"aggressive\",\n",
    "        strategy=None\n",
    "    )\n",
    "    flight.data[\"distance\"] = calculate_distances(flight.data) # meters\n",
    "    flight.data[\"groundspeed\"] = flight.data[\"distance\"] / 0.514444 # conversion to knot\n",
    "    bad_loc = (\n",
    "        (flight.data.altitude > 100) & (flight.data.groundspeed==0)\n",
    "    ).values.astype(bool)\n",
    "    flight.data = flight.data.loc[~bad_loc].dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Normalize timestamps to a continuous time interval with interpolation\n",
    "    flight.data = normalise_time_df(\n",
    "        flight.data, \n",
    "        time_freq=\"1s\", \n",
    "        time_column=\"timestamp\",\n",
    "        interpol_method=\"akima\",\n",
    "        cols_to_interpol=flight.data.columns[2:]\n",
    "    )\n",
    "    \n",
    "    flight.data[\"distance\"] = calculate_distances(flight.data) # meters\n",
    "    flight.data[\"groundspeed\"] = flight.data[\"distance\"] / 0.514444 # conversion to knot\n",
    "    flight.data[\"track\"] = calculate_bearings(flight.data)\n",
    "    flight.data.loc[flight.data[\"track\"]==0, \"track\"] = np.nan\n",
    "    flight.data[\"track\"] = flight.data[\"track\"].ffill().bfill()\n",
    "    flight.data[\"vertical_rate\"] = flight.data[\"altitude\"].diff().bfill() * 60 # per minute\n",
    "    \n",
    "    flight.data[\"track_x\"], flight.data[\"track_y\"] = np.sin(np.deg2rad(flight.data[\"track\"])), np.cos(np.deg2rad(flight.data[\"track\"]))\n",
    "    flight.data['wind_mag'] = np.sqrt(flight.data['wind_u']**2 + flight.data['wind_v']**2)\n",
    "    flight.data['wind_x'] = (flight.data['wind_u'] / flight.data['wind_mag']).fillna(0)\n",
    "    flight.data['wind_y'] = (flight.data['wind_v'] / flight.data['wind_mag']).fillna(0)\n",
    "    flight.data[\"track_wind_dot\"] = (flight.data['track_x'] * flight.data['wind_x']) + (flight.data['track_y'] * flight.data['wind_y'])\n",
    "    \n",
    "    # Compute true airspeed (TAS) and associated speed metrics\n",
    "    flight = flight.compute_TAS()\n",
    "    flight.data[\"TdG_speed\"] = (flight.data[\"TAS\"] - flight.data[\"groundspeed\"])\n",
    "    flight.data[\"ToG_speed\"] = (flight.data[\"TAS\"] / flight.data[\"groundspeed\"]).replace({np.inf: 0, -np.inf: 0})\n",
    "    flight.data[\"heading_x\"], flight.data[\"heading_y\"] = np.sin(np.deg2rad(flight.data[\"heading\"])), np.cos(np.deg2rad(flight.data[\"heading\"]))\n",
    "    flight.data[\"track_heading_dot\"] = (flight.data['track_x'] * flight.data['heading_x']) + (flight.data['track_y'] * flight.data['heading_y'])\n",
    "    flight.data[\"heading_wind_dot\"] = (flight.data['heading_x'] * flight.data['wind_x']) + (flight.data['heading_y'] * flight.data['wind_y'])\n",
    "    \n",
    "    flight = flight.phases()\n",
    "    flight.data = flight.data.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Generate phase segments and fixing LEVEL bugs\n",
    "    phase_segments = generate_phase_segments(flight.data)\n",
    "    if \"LEVEL\" in phase_segments:\n",
    "        for level_indexs in phase_segments[\"LEVEL\"]:\n",
    "            first_lvl_idx = level_indexs[0]\n",
    "            last_lvl_idx = level_indexs[-1]\n",
    "\n",
    "            if first_lvl_idx > 0:\n",
    "                previous_phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c3156f053649bbb1852497cfb77487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Parallel(n_jobs=12)( # 12 is max on my 64Gb RAM laptop without\n",
    "    delayed(         # making it impossible to use the laptop\n",
    "        process_parquet_parallel\n",
    "    )(\n",
    "        parquet_path=parquet_path, \n",
    "        min_flight_duration_minutes=1,\n",
    "        complete_flights=complete_flights,\n",
    "        usable_flight_ids=usable_flight_ids\n",
    "    )\n",
    "    for parquet_path in tqdm(parquet_paths)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
