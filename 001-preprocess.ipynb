{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from traffic.core import Flight\n",
    "from traffic.core import Traffic\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "flights_folder = os.path.join(os.getcwd(), \"flightDfs\")\n",
    "cha_df = pd.read_csv(os.path.join(data_folder, \"challenge_set.csv\"))\n",
    "sub_df = pd.read_csv(os.path.join(data_folder, \"submission_set.csv\"))\n",
    "final_sub_df = pd.read_csv(os.path.join(data_folder, \"final_submission_set.csv\"))\n",
    "\n",
    "parquet_paths = sorted([\n",
    "    os.path.join(data_folder, file) for file in os.listdir(data_folder) if \".parquet\" in file\n",
    "])\n",
    "\n",
    "usable_flight_ids = list(set(cha_df.flight_id.unique()).union(\n",
    "    set(sub_df.flight_id.unique()).union(\n",
    "        set(final_sub_df.flight_id.unique())\n",
    "    )\n",
    "))\n",
    "del cha_df, sub_df, final_sub_df\n",
    "\n",
    "if not os.path.isdir(flights_folder):\n",
    "    os.mkdir(flights_folder)\n",
    "\n",
    "complete_flights=[\n",
    "    file for file in os.listdir(flights_folder) if \".\" not in file\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_time_df(df, time_freq=\"1s\", interpol_method=\"akima\", time_column=\"timestamp\", cols_to_interpol=[]):\n",
    "    group_df = df.copy()\n",
    "    group_df[time_column] = pd.to_datetime(group_df[time_column])  # Ensure time_column is a datetime column\n",
    "    group_df = group_df.sort_values(by=\"timestamp\")\n",
    "\n",
    "    static_columns = group_df.columns.tolist()\n",
    "    static_columns.remove(time_column)\n",
    "    [static_columns.remove(interpol_column) for interpol_column in cols_to_interpol]\n",
    "    \n",
    "    # Generate a new DataFrame with a continuous 1-second interval for timestamps\n",
    "    start_time = group_df[time_column].min()\n",
    "    end_time = group_df[time_column].max()\n",
    "    \n",
    "    # Create a new range of timestamps at 1 second intervals\n",
    "    new_timestamps = pd.date_range(start=start_time, end=end_time, freq=time_freq)\n",
    "    \n",
    "    # Reindex the dataframe with the new timestamps\n",
    "    group_df.set_index(time_column, inplace=True)  # Set time_column as index for reindexing\n",
    "    group_df = group_df.reindex(new_timestamps)  # Reindex to include every second\n",
    "    \n",
    "    # Interpolating columns\n",
    "    group_df[cols_to_interpol] = group_df[cols_to_interpol].interpolate(method=interpol_method)\n",
    "\n",
    "    # Forward fill constant values\n",
    "    group_df[static_columns] = group_df[static_columns].ffill()\n",
    "    \n",
    "    # Reset the index to make time_column a column again\n",
    "    group_df.reset_index(inplace=True)\n",
    "    group_df = group_df.rename(columns={'index': time_column})\n",
    "\n",
    "    return group_df\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Compute the great-circle distance between two points on the Earth's surface using NumPy, in meters.\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Radius of earth in meters (6371 km converted to meters)\n",
    "    r = 6371000\n",
    "    return c * r\n",
    "\n",
    "def calculate_distances(df):\n",
    "    # Shift the lat/lon columns to create the \"previous\" row\n",
    "    lat1 = df['latitude'][:-1].values\n",
    "    lon1 = df['longitude'][:-1].values\n",
    "    lat2 = df['latitude'][1:].values\n",
    "    lon2 = df['longitude'][1:].values\n",
    "    \n",
    "    # Compute the distance between consecutive points\n",
    "    distances = haversine_np(lon1, lat1, lon2, lat2)\n",
    "    \n",
    "    # Append a NaN for the first distance (as there's no previous point for the first row)\n",
    "    distances = np.insert(distances, 0, np.nan)\n",
    "    \n",
    "    # Add distances to DataFrame in meters\n",
    "    return distances\n",
    "\n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the bearing (track angle) between two points using latitude and longitude.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "    \n",
    "    # Calculate the change in longitude\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Calculate bearing using the formula\n",
    "    x = np.sin(dlon) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "    \n",
    "    initial_bearing = np.arctan2(x, y)\n",
    "    \n",
    "    # Convert from radians to degrees\n",
    "    initial_bearing = np.degrees(initial_bearing)\n",
    "    \n",
    "    # Normalize the bearing to 0 - 360 degrees\n",
    "    bearing = (initial_bearing + 360) % 360\n",
    "    \n",
    "    return bearing\n",
    "\n",
    "def calculate_bearings(df):\n",
    "    # Shift the lat/lon columns to create the \"previous\" row\n",
    "    lat1 = df['latitude'][:-1].values\n",
    "    lon1 = df['longitude'][:-1].values\n",
    "    lat2 = df['latitude'][1:].values\n",
    "    lon2 = df['longitude'][1:].values\n",
    "    \n",
    "    # Compute the bearing between consecutive points\n",
    "    bearings = calculate_bearing(lat1, lon1, lat2, lon2)\n",
    "    \n",
    "    # Append a NaN for the first row as there's no previous point for it\n",
    "    bearings = np.insert(bearings, 0, np.nan)\n",
    "    \n",
    "    return bearings\n",
    "\n",
    "def generate_phase_segments(df):\n",
    "    df = df[[\"phase\"]].copy()\n",
    "    # Create a shifted phase column to detect phase changes\n",
    "    df['shifted_phase'] = df['phase'].shift(1)\n",
    "\n",
    "    # Mark where phase changes (start of a new segment)\n",
    "    df['segment'] = (df['phase'] != df['shifted_phase']).cumsum()\n",
    "\n",
    "    # Initialize the dictionary to store the segments\n",
    "    phase_segments = {}\n",
    "\n",
    "    # Use groupby to group by phase and segment and collect indices\n",
    "    for (phase, segment), group in df.groupby(['phase', 'segment']):\n",
    "        if phase not in phase_segments:\n",
    "            phase_segments[phase] = []\n",
    "        phase_segments[phase].append(np.array(list(group.index)))\n",
    "\n",
    "    return phase_segments\n",
    "\n",
    "def preprocess_flight(flight, output_folder=\"flights_folder\"):\n",
    "    flight.data = flight.data.drop(columns=[\"icao24\"]).rename(\n",
    "        columns={\n",
    "            \"u_component_of_wind\": \"wind_u\",\n",
    "            \"v_component_of_wind\": \"wind_v\"\n",
    "        }\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    flight = flight.filter(\n",
    "        filter=\"aggressive\",\n",
    "        strategy=None\n",
    "    )\n",
    "    flight.data[\"distance\"] = calculate_distances(flight.data) # meters\n",
    "    flight.data[\"groundspeed\"] = flight.data[\"distance\"] / 0.514444 # conversion to knot\n",
    "    bad_loc = (\n",
    "        (flight.data.altitude > 100) &\n",
    "        (flight.data.groundspeed==0)\n",
    "    ).values.astype(bool)\n",
    "    flight.data = flight.data.loc[~bad_loc].dropna().reset_index(drop=True)\n",
    "    \n",
    "    flight.data = normalise_time_df(\n",
    "        flight.data, \n",
    "        time_freq=\"1s\", \n",
    "        time_column=\"timestamp\",\n",
    "        interpol_method=\"akima\",\n",
    "        cols_to_interpol=flight.data.columns[2:]\n",
    "    )\n",
    "    \n",
    "    flight.data[\"distance\"] = calculate_distances(flight.data) # meters\n",
    "    flight.data[\"groundspeed\"] = flight.data[\"distance\"] / 0.514444 # conversion to knot\n",
    "    flight.data[\"track\"] = calculate_bearings(flight.data)\n",
    "    flight.data.loc[flight.data[\"track\"]==0, \"track\"] = np.nan\n",
    "    flight.data[\"track\"] = flight.data[\"track\"].ffill().bfill()\n",
    "    flight.data[\"vertical_rate\"] = flight.data[\"altitude\"].diff().bfill() * 60 # per minute\n",
    "    \n",
    "    flight.data[\"track_x\"], flight.data[\"track_y\"] = np.sin(np.deg2rad(flight.data[\"track\"])), np.cos(np.deg2rad(flight.data[\"track\"]))\n",
    "    flight.data['wind_mag'] = np.sqrt(flight.data['wind_u']**2 + flight.data['wind_v']**2)\n",
    "    flight.data['wind_x'] = (flight.data['wind_u'] / flight.data['wind_mag']).fillna(0)\n",
    "    flight.data['wind_y'] = (flight.data['wind_v'] / flight.data['wind_mag']).fillna(0)\n",
    "    flight.data[\"track_wind_dot\"] = (flight.data['track_x'] * flight.data['wind_x']) + (flight.data['track_y'] * flight.data['wind_y'])\n",
    "    \n",
    "    flight = flight.compute_TAS()\n",
    "    flight.data[\"TdG_speed\"] = (flight.data[\"TAS\"] - flight.data[\"groundspeed\"])\n",
    "    flight.data[\"ToG_speed\"] = (flight.data[\"TAS\"] / flight.data[\"groundspeed\"]).replace({np.inf: 0, -np.inf: 0})\n",
    "    flight.data[\"heading_x\"], flight.data[\"heading_y\"] = np.sin(np.deg2rad(flight.data[\"heading\"])), np.cos(np.deg2rad(flight.data[\"heading\"]))\n",
    "    flight.data[\"track_heading_dot\"] = (flight.data['track_x'] * flight.data['heading_x']) + (flight.data['track_y'] * flight.data['heading_y'])\n",
    "    flight.data[\"heading_wind_dot\"] = (flight.data['heading_x'] * flight.data['wind_x']) + (flight.data['heading_y'] * flight.data['wind_y'])\n",
    "    \n",
    "    flight = flight.phases()\n",
    "    flight.data = flight.data.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Generate phase segments and fixing LEVEL bugs\n",
    "    phase_segments = generate_phase_segments(flight.data)\n",
    "    if \"LEVEL\" in phase_segments:\n",
    "        for level_indexs in phase_segments[\"LEVEL\"]:\n",
    "            first_lvl_idx = level_indexs[0]\n",
    "            last_lvl_idx = level_indexs[-1]\n",
    "\n",
    "            if first_lvl_idx > 0:\n",
    "                previous_phase = flight.data.loc[first_lvl_idx-1, \"phase\"]\n",
    "                upcoming_phase = flight.data.loc[last_lvl_idx+1, \"phase\"]\n",
    "\n",
    "                # this if statment deals with LEVEL segments which occur\n",
    "                # between 2 different segments, and assumes that\n",
    "                # a LEVEL segment should only occur between two identical segments\n",
    "                # e.g., CRUISE - LEVEL - CRUISE   is OK\n",
    "                #       CRUISE - LEVEL - DESCENT  is NOT OK\n",
    "                # when NOT OK, LEVEL is converted into the previous phase\n",
    "                if previous_phase != upcoming_phase:\n",
    "                    flight.data.loc[level_indexs, \"phase\"] = previous_phase\n",
    "\n",
    "                else:\n",
    "                    previous_segment_index = np.argmax([\n",
    "                        (first_lvl_idx-1) in phase_segment \n",
    "                        for phase_segment in phase_segments[previous_phase]\n",
    "                    ])\n",
    "                    current_segment_length = len(level_indexs)\n",
    "                    previous_segment_length = len(phase_segments[previous_phase][previous_segment_index])\n",
    "                    upcoming_segment_length = len(phase_segments[previous_phase][previous_segment_index+1])\n",
    "                    # if the twho \"before and after\" segments are identical,\n",
    "                    # check if the LEVEL length in question is GREATER\n",
    "                    # than both segments, assumming that LEVEL segments \n",
    "                    # should be smaller segments within phases\n",
    "                    # if LEVEL segment is too large, then consider it a bug and convert it into the phase it is contained in\n",
    "                    if (current_segment_length >= previous_segment_length) and (current_segment_length >= upcoming_segment_length):\n",
    "                        flight.data.loc[level_indexs, \"phase\"] = previous_phase\n",
    "    \n",
    "    if \"GROUND\" in flight.data.phase.unique():\n",
    "        last_ground_index = flight.data.loc[flight.data.phase==\"GROUND\"].index[-1]\n",
    "        # this if statement makes sure that we don't exclude the entire flight\n",
    "        # in the case where GROUND only appears at the beggining of the data\n",
    "        # since then we would discard all of the acctual flight\n",
    "        if last_ground_index >= int(0.25 * len(flight.data)): \n",
    "            flight.data = flight.data.loc[:last_ground_index].reset_index(drop=True)\n",
    "    \n",
    "    joblib.dump(\n",
    "        flight.data,\n",
    "        os.path.join(output_folder, str(int(flight.flight_id)))\n",
    "    )\n",
    "\n",
    "def process_parquet_parallel(parquet_path=\"path\", complete_flights=\"list\", usable_flight_ids=\"list\", min_flight_duration_minutes=1):\n",
    "    traffics = Traffic.from_file(parquet_path)\n",
    "    for traffic in tqdm(traffics, leave=False, desc=parquet_path.split(\"\\\\\")[-1].split(\".\")[0]):\n",
    "        if (traffic.flight_id in usable_flight_ids):\n",
    "            flight = Flight(traffic.data)\n",
    "            try:\n",
    "                flight.data[\"timestamp\"] = pd.to_datetime(flight.data[\"timestamp\"])\n",
    "                flight_duration_minutes = (flight.data.timestamp.max() - flight.data.timestamp.min()).total_seconds() // 60\n",
    "                if (flight_duration_minutes >= min_flight_duration_minutes) and (str(int(flight.flight_id)) not in complete_flights):\n",
    "                    preprocess_flight(flight, output_folder=flights_folder)\n",
    "            except Exception as e:\n",
    "                print(e, parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c3156f053649bbb1852497cfb77487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Parallel(n_jobs=12)( # 12 is max on my 64Gb RAM laptop without\n",
    "    delayed(         # making it impossible to use the laptop\n",
    "        process_parquet_parallel\n",
    "    )(\n",
    "        parquet_path=parquet_path, \n",
    "        min_flight_duration_minutes=1,\n",
    "        complete_flights=complete_flights,\n",
    "        usable_flight_ids=usable_flight_ids\n",
    "    )\n",
    "    for parquet_path in tqdm(parquet_paths)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
