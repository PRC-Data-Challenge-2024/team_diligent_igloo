{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a75d09-62db-44a9-9b46-345d49ed05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import joblib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "from traffic.core import Flight\n",
    "from traffic.core import Traffic\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "runway_df = pd.read_csv(\"runways.csv\")\n",
    "airports_df = pd.read_csv(\"airports.csv\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "acropole_folder = \"../../../Acropole/acropole/\"\n",
    "flights_folder = os.path.join(os.getcwd(), \"flightDfs\")\n",
    "cha_df = pd.read_csv(os.path.join(data_folder, \"challenge_set.csv\"))\n",
    "sub_df = pd.read_csv(os.path.join(data_folder, \"submission_set.csv\"))\n",
    "final_sub_df = pd.read_csv(os.path.join(data_folder, \"final_submission_set.csv\"))\n",
    "acropole_folder = \"../../../Acropole/acropole/data/\"\n",
    "flight_fuel_df = joblib.load(\"flight_fuel_df.pkl\")\n",
    "usable_flight_ids = list(set(cha_df.flight_id.unique()).union(\n",
    "    set(sub_df.flight_id.unique()).union(\n",
    "        set(final_sub_df.flight_id.unique())\n",
    "    )\n",
    "))\n",
    "\n",
    "airport_df = joblib.load(\"airport_df.pkl\")\n",
    "aircraft_df = joblib.load(\"aircraft_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e4ec8a1-b644-4e42-9847-d36a4b95d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_corr_columns(corr_df, corr_th=.7):\n",
    "    mezo_corr = corr_df.abs()\n",
    "    while (mezo_corr >= corr_th).sum().sum() > len(mezo_corr):\n",
    "        columns_over_corr_counts = (mezo_corr >= corr_th).sum()\n",
    "        if columns_over_corr_counts.sum() > len(mezo_corr):\n",
    "            col_to_remove = mezo_corr.columns[\n",
    "                columns_over_corr_counts.argmax()\n",
    "            ]\n",
    "            mezo_corr = mezo_corr.drop(columns=col_to_remove).drop(index=col_to_remove)\n",
    "\n",
    "    low_corr_columns = np.array(mezo_corr.columns)\n",
    "    \n",
    "    return low_corr_columns\n",
    "\n",
    "def is_leap_year(year):\n",
    "    \"\"\"\n",
    "    Determines if a given year is a leap year.\n",
    "\n",
    "    Parameters:\n",
    "    year (int): The year to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the year is a leap year, False otherwise.\n",
    "    \"\"\"\n",
    "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
    "\n",
    "def day_cyclic_values(timestamps):\n",
    "    \"\"\"\n",
    "    Computes cyclic values (x, y) for time of day using a unit circle.\n",
    "\n",
    "    Parameters:\n",
    "    timestamps (pd.Series): Timestamps for which to compute the cyclic values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays representing the x and y coordinates on a unit circle.\n",
    "    \"\"\"\n",
    "    second_of_day = timestamps.dt.hour * 3600 + timestamps.dt.minute * 60 + timestamps.dt.second\n",
    "    second_of_day_normalized = second_of_day / 86400.0  # 86400 seconds in a day\n",
    "    x_day = np.cos(2 * np.pi * second_of_day_normalized)\n",
    "    y_day = np.sin(2 * np.pi * second_of_day_normalized)\n",
    "    return x_day, y_day\n",
    "\n",
    "def week_cyclic_values(timestamps):\n",
    "    \"\"\"\n",
    "    Computes cyclic values (x, y) for the day of the week using a unit circle.\n",
    "\n",
    "    Parameters:\n",
    "    timestamps (pd.Series): Timestamps for which to compute the cyclic values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays representing the x and y coordinates on a unit circle.\n",
    "    \"\"\"\n",
    "    day_of_week = timestamps.dt.dayofweek\n",
    "    second_of_day = timestamps.dt.hour * 3600 + timestamps.dt.minute * 60 + timestamps.dt.second\n",
    "    day_of_week_normalized = day_of_week / 7.0\n",
    "    second_of_day_normalized = second_of_day / 86400.0  # 86400 seconds in a day\n",
    "    week_fraction = day_of_week_normalized + second_of_day_normalized / 7.0\n",
    "    x_week = np.cos(2 * np.pi * week_fraction)\n",
    "    y_week = np.sin(2 * np.pi * week_fraction)\n",
    "    return x_week, y_week\n",
    "\n",
    "def month_cyclic_values(timestamps):\n",
    "    \"\"\"\n",
    "    Computes cyclic values (x, y) for the day of the month using a unit circle.\n",
    "\n",
    "    Parameters:\n",
    "    timestamps (pd.Series): Timestamps for which to compute the cyclic values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays representing the x and y coordinates on a unit circle.\n",
    "    \"\"\"\n",
    "    day_of_month = timestamps.dt.day\n",
    "    second_of_day = timestamps.dt.hour * 3600 + timestamps.dt.minute * 60 + timestamps.dt.second\n",
    "    days_in_month = timestamps.dt.days_in_month\n",
    "    day_of_month_normalized = day_of_month / days_in_month\n",
    "    second_of_day_normalized = second_of_day / 86400.0  # 86400 seconds in a day\n",
    "    month_fraction = day_of_month_normalized + second_of_day_normalized / days_in_month\n",
    "    x_month = np.cos(2 * np.pi * month_fraction)\n",
    "    y_month = np.sin(2 * np.pi * month_fraction)\n",
    "    return x_month, y_month\n",
    "\n",
    "def year_cyclic_values(timestamps):\n",
    "    \"\"\"\n",
    "    Computes cyclic values (x, y) for the day of the year using a unit circle, accounting for leap years.\n",
    "\n",
    "    Parameters:\n",
    "    timestamps (pd.Series): Timestamps for which to compute the cyclic values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays representing the x and y coordinates on a unit circle.\n",
    "    \"\"\"\n",
    "    year = timestamps.dt.year\n",
    "    day_of_year = timestamps.dt.dayofyear\n",
    "    second_of_day = timestamps.dt.hour * 3600 + timestamps.dt.minute * 60 + timestamps.dt.second\n",
    "    days_in_year = year.apply(lambda y: 366 if is_leap_year(y) else 365)\n",
    "    day_of_year_normalized = day_of_year / days_in_year\n",
    "    second_of_day_normalized = second_of_day / 86400.0  # 86400 seconds in a day\n",
    "    year_fraction = day_of_year_normalized + second_of_day_normalized / days_in_year\n",
    "    x_year = np.cos(2 * np.pi * year_fraction)\n",
    "    y_year = np.sin(2 * np.pi * year_fraction)\n",
    "    return x_year, y_year\n",
    "\n",
    "def generate_phase_segments(df):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into segments where the 'phase' column changes.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame containing a 'phase' column.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are phase values and values are lists of indices for each segment.\n",
    "    \"\"\"\n",
    "    df = df[[\"phase\"]].copy()\n",
    "    # Create a shifted phase column to detect phase changes\n",
    "    df['shifted_phase'] = df['phase'].shift(1)\n",
    "\n",
    "    # Mark where phase changes (start of a new segment)\n",
    "    df['segment'] = (df['phase'] != df['shifted_phase']).cumsum()\n",
    "\n",
    "    # Initialize the dictionary to store the segments\n",
    "    phase_segments = {}\n",
    "\n",
    "    # Use groupby to group by phase and segment and collect indices\n",
    "    for (phase, segment), group in df.groupby(['phase', 'segment']):\n",
    "        if phase not in phase_segments:\n",
    "            phase_segments[phase] = []\n",
    "        phase_segments[phase].append(np.array(list(group.index)))\n",
    "\n",
    "    return phase_segments\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Compute the great-circle distance between two points on the Earth's surface using NumPy, in meters.\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Radius of earth in meters (6371 km converted to meters)\n",
    "    r = 6371000\n",
    "    return c * r\n",
    "\n",
    "def calculate_distances(df, lat1_col, lon1_col, lat2_col, lon2_col):\n",
    "    # Shift the lat/lon columns to create the \"previous\" row\n",
    "    lat1 = df[lat1_col].values\n",
    "    lon1 = df[lon1_col].values\n",
    "    lat2 = df[lat2_col].values\n",
    "    lon2 = df[lon2_col].values\n",
    "    \n",
    "    # Compute the distance between consecutive points\n",
    "    distances = haversine_np(lon1, lat1, lon2, lat2)\n",
    "\n",
    "    # Add distances to DataFrame in meters\n",
    "    return distances\n",
    "\n",
    "def get_flight_stats_row(flight_df=\"df\", n_quantiles=5, stats_columns=\"list\"):\n",
    "    \"\"\"\n",
    "    Computes statistics (mean, std, quantiles) and cyclic variability for flight data.\n",
    "\n",
    "    Parameters:\n",
    "    flight_df (pd.DataFrame): DataFrame containing flight data.\n",
    "    n_quantiles (int): Number of quantiles to compute.\n",
    "    stats_columns (list): List of column names to compute statistics on.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A series of computed statistics including mean, standard deviation, quantiles, \n",
    "               and variability within quantile bins.\n",
    "    \"\"\"\n",
    "    means = flight_df[stats_columns].mean()\n",
    "    stdvs = flight_df[stats_columns].std(ddof=0)\n",
    "    quants = flight_df[stats_columns].quantile(q=np.linspace(0,1,n_quantiles).round(2))\n",
    "    total_distance = pd.Series(flight_df.distance.sum(), index=[\"total_distance\"])\n",
    "    # rearranging\n",
    "    means.index = [\"mean_\" + ind for ind in means.index]\n",
    "    stdvs.index = [\"stdv_\" + ind for ind in stdvs.index]\n",
    "    quants = quants.stack().to_frame().T\n",
    "    quants.columns = [f\"{col[0]}_{col[1]}\" for col in quants.columns]\n",
    "    quants = quants.loc[0]\n",
    "    \n",
    "    # Step 1: Prepare dictionaries to hold standard deviations and normalized standard deviations\n",
    "    quantile_stdvs_dict = {}\n",
    "    normalized_stdvs_dict = {}\n",
    "    \n",
    "    # Step 2: Loop over each stat_column\n",
    "    for column in stats_columns:\n",
    "        # Get the quantile values for the current column, ensuring duplicates are dropped\n",
    "        quantile_bins = flight_df[column].quantile(q=np.linspace(0, 1, n_quantiles)).values\n",
    "        unique_quantile_bins = np.unique(quantile_bins)  # Remove duplicate quantile values\n",
    "        \n",
    "        # Digitize the data points into the unique quantile bins\n",
    "        bin_labels = np.arange(1, len(unique_quantile_bins))  # Adjust labels for the available unique bins\n",
    "        binned_data = pd.cut(flight_df[column], bins=unique_quantile_bins, labels=bin_labels, include_lowest=True)\n",
    "        \n",
    "        # Compute the standard deviation for each unique bin\n",
    "        if len(unique_quantile_bins)==1:\n",
    "            for i in range(1, n_quantiles):\n",
    "                quantile_stdvs_dict[f'{column}_quant_{i}_stdv'] = stdvs[f'stdv_{column}']\n",
    "        else:\n",
    "            for i in range(1, len(unique_quantile_bins)):\n",
    "                bin_stdv = flight_df[column][binned_data == i].std()\n",
    "                quantile_stdvs_dict[f'{column}_quant_{i}_stdv'] = bin_stdv\n",
    "        \n",
    "            # If there are fewer quantiles than desired, duplicate the last bin's standard deviation        \n",
    "            last_stdv = quantile_stdvs_dict[f'{column}_quant_{len(unique_quantile_bins)-1}_stdv']\n",
    "            for i in range(len(unique_quantile_bins), n_quantiles):\n",
    "                quantile_stdvs_dict[f'{column}_quant_{i}_stdv'] = last_stdv\n",
    "        \n",
    "        # Normalize the standard deviations by dividing by the absolute mean (handling division by zero)\n",
    "        mean_value = abs(means[f'mean_{column}'])\n",
    "        for i in range(1, n_quantiles):\n",
    "            stdv_col_name = f'{column}_quant_{i}_stdv'\n",
    "            stdv_value = quantile_stdvs_dict[stdv_col_name]\n",
    "            normalized_value = stdv_value / mean_value if mean_value != 0 else -1\n",
    "            normalized_stdvs_dict[f'{column}_quant_{i}_norm_stdv'] = normalized_value\n",
    "    \n",
    "    # Step 3: Convert the dictionaries to DataFrames\n",
    "    quantile_stdvs = pd.DataFrame(quantile_stdvs_dict, index=[0])\n",
    "    normalized_stdvs = pd.DataFrame(normalized_stdvs_dict, index=[0])\n",
    "    \n",
    "    # concatenating everything\n",
    "    flight_stats_row = pd.concat([\n",
    "        means, stdvs, quants, total_distance, quantile_stdvs.squeeze(), normalized_stdvs.squeeze()\n",
    "    ])\n",
    "\n",
    "    return flight_stats_row\n",
    "\n",
    "def get_flight_stats_df(flight_df=\"df\", n_quantiles=5, stats_columns=\"list\", possible_phases=\"list\"):\n",
    "    \"\"\"\n",
    "    Computes flight statistics for each phase and generates a DataFrame with the results.\n",
    "\n",
    "    Parameters:\n",
    "    flight_df (pd.DataFrame): DataFrame containing flight data.\n",
    "    n_quantiles (int): Number of quantiles to compute.\n",
    "    stats_columns (list): List of column names to compute statistics on.\n",
    "    possible_phases (list): List of possible flight phases to consider.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing statistics for each flight and phase.\n",
    "    \"\"\"\n",
    "    global_flight_stats = get_flight_stats_row(\n",
    "        flight_df=flight_df, \n",
    "        n_quantiles=n_quantiles,\n",
    "        stats_columns=stats_columns, \n",
    "    )\n",
    "    \n",
    "    all_phase_flight_stats = []\n",
    "    all_phases_dict = generate_phase_segments(flight_df)\n",
    "    for phase in possible_phases:\n",
    "        phase_df = flight_df.loc[flight_df.phase==phase]\n",
    "        if len(phase_df):\n",
    "            # global_phase_variability_stats\n",
    "            phase_flight_stats = get_flight_stats_row(\n",
    "                flight_df=phase_df,\n",
    "                n_quantiles=n_quantiles,\n",
    "                stats_columns=stats_columns,\n",
    "            )\n",
    "            n_phase_segments = len(all_phases_dict[phase])\n",
    "            phase_flight_stats = pd.concat([\n",
    "                pd.Series(\n",
    "                    data=[n_phase_segments],\n",
    "                    index=[\"n_segments\"],\n",
    "                ),\n",
    "                phase_flight_stats\n",
    "            ])\n",
    "            # inter_segment_phase_variability_stats\n",
    "            # how much change is there between segments in the same phase\n",
    "            if n_phase_segments > 1:\n",
    "                stdvs_segment_coefficients_of_variation = []\n",
    "                means_segment_coefficients_of_variation = []\n",
    "                for phase_segment_idxs in all_phases_dict[phase]:\n",
    "                    segment = phase_df.loc[phase_segment_idxs]\n",
    "                    segment_coefficients_of_variation = (\n",
    "                        segment[stats_columns].std(ddof=0) / abs(segment[stats_columns].mean())\n",
    "                    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "                    stdv_segment_coefficients_of_variation = segment_coefficients_of_variation.std(ddof=0)\n",
    "                    mean_segment_coefficients_of_variation = segment_coefficients_of_variation.mean()\n",
    "                    stdvs_segment_coefficients_of_variation.append(stdv_segment_coefficients_of_variation)\n",
    "                    means_segment_coefficients_of_variation.append(mean_segment_coefficients_of_variation)\n",
    "                \n",
    "                stdv_stdvs_segment_coefficients_of_variation = np.std(stdvs_segment_coefficients_of_variation, ddof=0)\n",
    "                stdv_means_segment_coefficients_of_variation = np.std(means_segment_coefficients_of_variation, ddof=0)\n",
    "                inter_segment_variability_stats = pd.Series(\n",
    "                    data=[\n",
    "                        stdv_stdvs_segment_coefficients_of_variation, \n",
    "                        stdv_means_segment_coefficients_of_variation\n",
    "                    ],\n",
    "                    index=[\n",
    "                        f\"{phase}_stdv_stdvs_segment_coefficients_of_variation\", \n",
    "                        f\"{phase}_stdv_means_segment_coefficients_of_variation\", \n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                inter_segment_variability_stats = pd.Series(\n",
    "                    data=[\n",
    "                        0, \n",
    "                        0\n",
    "                    ],\n",
    "                    index=[\n",
    "                        f\"{phase}_stdv_stdvs_segment_coefficients_of_variation\", \n",
    "                        f\"{phase}_stdv_means_segment_coefficients_of_variation\", \n",
    "                    ]\n",
    "                )\n",
    "        else:\n",
    "            phase_flight_stats = pd.Series(\n",
    "                np.repeat(np.nan, len(global_flight_stats)),\n",
    "                index=global_flight_stats.index\n",
    "            )\n",
    "            phase_flight_stats = pd.concat([\n",
    "                pd.Series(\n",
    "                    data=[0],\n",
    "                    index=[\"n_segments\"],\n",
    "                ),\n",
    "                phase_flight_stats\n",
    "            ])\n",
    "            inter_segment_variability_stats = pd.Series(\n",
    "                data=[\n",
    "                    np.nan, \n",
    "                    np.nan,\n",
    "                ],\n",
    "                index=[\n",
    "                    f\"{phase}_stdv_stdvs_segment_coefficients_of_variation\", \n",
    "                    f\"{phase}_stdv_means_segment_coefficients_of_variation\", \n",
    "                ]\n",
    "            )\n",
    "        phase_flight_stats.index = [f\"{phase}_{index}\" for index in phase_flight_stats.index]\n",
    "        all_phase_flight_stats.append(\n",
    "            pd.concat([\n",
    "                inter_segment_variability_stats,\n",
    "                phase_flight_stats\n",
    "            ])\n",
    "        )\n",
    "    all_phase_flight_stats = pd.concat(all_phase_flight_stats)\n",
    "    flight_stats = pd.concat([global_flight_stats, all_phase_flight_stats])\n",
    "    flight_stats_df = pd.DataFrame(flight_stats, columns=[int(flight_df.flight_id.unique())]).T\n",
    "    return flight_stats_df\n",
    "\n",
    "def process_flight_stats_batch(flight_ids_batch, flights_folder, n_quantiles, stats_columns, possible_phases):\n",
    "    \"\"\"\n",
    "    Processes a batch of flight IDs and computes statistics for each flight.\n",
    "\n",
    "    Parameters:\n",
    "    flight_ids_batch (list): List of flight IDs to process.\n",
    "    flights_folder (str): Path to the folder containing flight data.\n",
    "    n_quantiles (int): Number of quantiles to compute.\n",
    "    stats_columns (list): List of column names to compute statistics on.\n",
    "    possible_phases (list): List of possible flight phases to consider.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing statistics for all flights in the batch.\n",
    "    \"\"\"\n",
    "    flight_stats_df_list = []\n",
    "    for flight_id in flight_ids_batch:\n",
    "        flight_df = joblib.load(os.path.join(flights_folder, str(flight_id)))\n",
    "        flight_stats_df = get_flight_stats_df(\n",
    "            flight_df=flight_df, \n",
    "            n_quantiles=n_quantiles,\n",
    "            stats_columns=stats_columns, \n",
    "            possible_phases=possible_phases,\n",
    "        )\n",
    "        flight_stats_df_list.append(flight_stats_df)\n",
    "    flight_stats_df = pd.concat(flight_stats_df_list, axis=0)\n",
    "    return flight_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78338009-782b-4716-9074-b2f39e3f9d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5b5c9150094648891e197413132184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating test_set input for the model\n",
    "n_quantiles_list = [5]\n",
    "for n_quantiles in tqdm(n_quantiles_list):\n",
    "\n",
    "    # this will be used to determine one-hot-encodings\n",
    "    # for the categorical variables airlines and aircrafttypes\n",
    "    X_y_df_cha = joblib.load(\n",
    "        f\"X_y_df_cha_nq={n_quantiles}_v4.pkl\"\n",
    "    )\n",
    "    X_y_df = final_sub_df.copy()\n",
    "\n",
    "    X_y_df.arrival_time = pd.to_datetime(X_y_df.arrival_time)\n",
    "    X_y_df.actual_offblock_time = pd.to_datetime(X_y_df.actual_offblock_time)\n",
    "    \n",
    "    X_y_df[\"arrDayX\"], X_y_df[\"arrDayY\"] = day_cyclic_values(X_y_df.arrival_time)\n",
    "    X_y_df[\"arrWeekX\"], X_y_df[\"arrWeekY\"] = week_cyclic_values(X_y_df.arrival_time)\n",
    "    X_y_df[\"arrMonthX\"], X_y_df[\"arrMonthY\"] = month_cyclic_values(X_y_df.arrival_time)\n",
    "    X_y_df[\"arrYearX\"], X_y_df[\"arrYearY\"] = year_cyclic_values(X_y_df.arrival_time)\n",
    "    \n",
    "    X_y_df[\"depDayX\"], X_y_df[\"depDayY\"] = day_cyclic_values(X_y_df.actual_offblock_time)\n",
    "    X_y_df[\"depWeekX\"], X_y_df[\"depWeekY\"] = week_cyclic_values(X_y_df.actual_offblock_time)\n",
    "    X_y_df[\"depMonthX\"], X_y_df[\"depMonthY\"] = month_cyclic_values(X_y_df.actual_offblock_time)\n",
    "    X_y_df[\"depYearX\"], X_y_df[\"depYearY\"] = year_cyclic_values(X_y_df.actual_offblock_time)\n",
    "    \n",
    "    # add aircraft and airport point features\n",
    "    X_y_df = X_y_df.merge(right=aircraft_df, how=\"left\", left_on=\"aircraft_type\", right_index=True)\n",
    "    X_y_df = X_y_df.merge(right=airport_df, how=\"left\", left_on=\"adep\", right_index=True)\n",
    "    X_y_df = X_y_df.merge(right=airport_df, how=\"left\", left_on=\"ades\", right_index=True, suffixes=(\"_adep\", \"_ades\"))\n",
    "    X_y_df = X_y_df.merge(\n",
    "        right=flight_fuel_df.rename(columns={\n",
    "            \"total_fuel\": \"FUEL_total_consumption\",\n",
    "            \"total_fuel_replace\": \"FUEL_total_consumption_replace\",\n",
    "        }), \n",
    "        how=\"left\", left_on=\"flight_id\", right_on=\"flight_id\"\n",
    "    )\n",
    "    \n",
    "    # Processes the `X_y_df` DataFrame by consolidating less frequent airlines into an \"other\" category, \n",
    "    # and converting the `airline` column into one-hot encoded binary variables. \n",
    "    # This is done to reduce the number of airline categories and prepare the data for analysis or modeling.\n",
    "    encoded_airlines_columns = X_y_df_cha.columns[\n",
    "        X_y_df_cha.columns.str.contains(\"airline_\") &\n",
    "        ~(X_y_df_cha.columns.str.contains(\"airline_other\"))\n",
    "    ]\n",
    "    encoded_airlines = [item[1] for item in encoded_airlines_columns.str.split(\"airline_\")]\n",
    "    X_y_df.loc[~X_y_df.airline.isin(encoded_airlines).values.astype(bool), \"airline\"] = \"other\"\n",
    "    airline_dummie_df = pd.get_dummies(X_y_df.airline).astype(float)\n",
    "    airline_dummie_df.columns = [\"AIRLINE_airline_\" + column for column in airline_dummie_df.columns]\n",
    "    X_y_df = pd.concat([X_y_df, airline_dummie_df], axis=1)\n",
    "    \n",
    "    encoded_actypes_columns = X_y_df_cha.columns[\n",
    "        X_y_df_cha.columns.str.contains(\"aircraft_type\") &\n",
    "        ~(X_y_df_cha.columns.str.contains(\"aircraft_type_other\"))\n",
    "    ]\n",
    "    encoded_actypes = [item[1] for item in encoded_actypes_columns.str.split(\"_type_\")]\n",
    "    X_y_df.loc[~X_y_df.aircraft_type.isin(encoded_actypes).values.astype(bool), \"aircraft_type\"] = \"other\"\n",
    "    aircraft_type_dummie_df = pd.get_dummies(X_y_df.aircraft_type).astype(float)\n",
    "    aircraft_type_dummie_df.columns = [\"AIRCRAFT_aircraft_type_\" + column for column in aircraft_type_dummie_df.columns]\n",
    "    X_y_df = pd.concat([X_y_df, aircraft_type_dummie_df], axis=1)\n",
    "    \n",
    "    X_y_df = X_y_df.drop(columns=[\n",
    "        # \"flight_id\", \"date\", \n",
    "        \"callsign\", \"wtc\", \"airline\", \n",
    "        \"name_adep\", \"country_code_adep\", \n",
    "        \"name_ades\", \"country_code_ades\",\n",
    "        \"actual_offblock_time\", \"arrival_time\",\n",
    "        \"aircraft_type\", \"adep\", \"ades\"\n",
    "    ])\n",
    "    \n",
    "    # generate new airport features based on the difference \n",
    "    # between attribute values of departure and arrival airports\n",
    "    adep_columns = [column for column in X_y_df.columns if \"_adep\" in column]\n",
    "    ades_columns = [column for column in X_y_df.columns if \"_ades\" in column]\n",
    "    for i in range(len(adep_columns)):\n",
    "        adep_column = adep_columns[i]\n",
    "        ades_column = ades_columns[i]\n",
    "        diff_col_name = f\"{adep_column}_diff\"\n",
    "        X_y_df[diff_col_name] = X_y_df[ades_column] - X_y_df[adep_column]    \n",
    "        \n",
    "    X_y_df[\"AIRPORT_airports_distance\"] = calculate_distances(\n",
    "        X_y_df,\n",
    "        lat1_col=\"AIRPORT_latitude_adep\", \n",
    "        lon1_col=\"AIRPORT_longitude_adep\", \n",
    "        lat2_col=\"AIRPORT_latitude_ades\", \n",
    "        lon2_col=\"AIRPORT_longitude_ades\"\n",
    "    )\n",
    "    \n",
    "    # add flight features\n",
    "    flight_stats_df = joblib.load(f\"flight_stats_df_nq={n_quantiles}.pkl\")\n",
    "    X_y_df = X_y_df.merge(right=flight_stats_df, how=\"left\", left_on=\"flight_id\", right_index=True)\n",
    "    \n",
    "    column_order = X_y_df.columns.tolist()\n",
    "    column_order.remove(\"tow\")\n",
    "    column_order += [\"tow\"]\n",
    "    X_y_df = X_y_df[column_order]\n",
    "    # filling nan values\n",
    "    feature_columns = X_y_df.columns[2:-1]\n",
    "    for feature_column in feature_columns:\n",
    "        if X_y_df[feature_column].isna().sum():\n",
    "            # replacing nan with nan_placeholder from X_y_df_cha\n",
    "            nan_placeholder = X_y_df_cha[feature_column].min()\n",
    "            X_y_df[feature_column] = X_y_df[feature_column].fillna(nan_placeholder)\n",
    "    \n",
    "    joblib.dump(X_y_df, f\"X_y_df_final_sub_nq={n_quantiles}_v4.pkl\")\n",
    "\n",
    "del X_y_df_cha, X_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05b1d9ed-59b3-46b2-9a94-7e5144748442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fd9cba83a74a809f232091bdde2e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create corr_df for column dropping based on correlation threshold\n",
    "for n_quantiles in tqdm(n_quantiles_list):\n",
    "    if f\"X_y_final_sub_nq={n_quantiles}_corr_df_v4.pkl\" not in os.listdir(os.getcwd()):\n",
    "        X_y_df = joblib.load(f\"X_y_df_final_sub_nq={n_quantiles}_v4.pkl\")\n",
    "        feature_columns = X_y_df.columns[2:-1]\n",
    "        cat_features = [\n",
    "            column for column in feature_columns if (\n",
    "                (\"airline_\" in column) or (\"aircraft_type_\" in column)\n",
    "            )\n",
    "        ]\n",
    "        num_features = sorted(\n",
    "            list(set(feature_columns).difference(set(cat_features)))\n",
    "        )\n",
    "        corr_df = X_y_df[num_features].corr()\n",
    "        joblib.dump(corr_df, f\"X_y_final_sub_nq={n_quantiles}_corr_df_v4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "638c94fa-94c3-49cc-aa70-560fd1a965de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_xgb_X_y_df_final_sub_v4.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = joblib.load(\"results_df_v4.pkl\")\n",
    "learning_rate, n_estimators, max_depth, col_sample, corr_th, n_quantiles = results_df.sort_values(\n",
    "    by=\"RMSE_avg\"\n",
    ").iloc[0].values[:6]\n",
    "\n",
    "# cha\n",
    "X_y_cha_df = joblib.load(f\"X_y_df_cha_nq={n_quantiles}_v4.pkl\")\n",
    "corr_cha_df = joblib.load(f\"X_y_cha_nq={n_quantiles}_corr_df_v4.pkl\")\n",
    "cat_features = [\n",
    "    column for column in X_y_cha_df.columns[2:-1] if (\n",
    "        (\"airline_\" in column) or (\"aircraft_type_\" in column)\n",
    "    )\n",
    "]\n",
    "low_corr_columns = get_low_corr_columns(corr_cha_df, corr_th).tolist() + cat_features\n",
    "id_columns = X_y_cha_df.columns[:2]\n",
    "joblib.dump(\n",
    "    X_y_cha_df[[id_columns[0]] + low_corr_columns + [\"tow\"]],\n",
    "    \"best_xgb_X_y_df_cha_v4.pkl\"\n",
    ")\n",
    "\n",
    "# final sub\n",
    "X_y_final_sub_df = joblib.load(f\"X_y_df_final_sub_nq={n_quantiles}_v4.pkl\")\n",
    "joblib.dump(\n",
    "    X_y_final_sub_df[[id_columns[0]] + low_corr_columns + [\"tow\"]],\n",
    "    \"best_xgb_X_y_df_final_sub_v4.pkl\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa069e-9b7c-4050-bfb4-f2204fb6e820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03fd9cba83a74a809f232091bdde2e35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a887b64077b043f1a069dce02394916b",
        "IPY_MODEL_20cb66e5914c4ed99fe1aaef541bfa41",
        "IPY_MODEL_39a8d271178e4bfaa9298f17ca4f5690"
       ],
       "layout": "IPY_MODEL_5b8a5042c03042d68b3781fdfb4e6f52"
      }
     },
     "045f30c43ee04a18acd4a6f8719c471b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "102f49e793614c15aa250675e01c245a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dd63bd5b54684233a8233ab8221a09b9",
       "style": "IPY_MODEL_8f5ebb7e70494b008a72768438eb7d92",
       "value": " 1/1 [00:16&lt;00:00, 16.33s/it]"
      }
     },
     "11546c4ee3584559a99a9fd15044b97d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "14626b858c814dc1a8171732859c869b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "20cb66e5914c4ed99fe1aaef541bfa41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_e57d1d8422b94ffc808c96f405f1c56a",
       "max": 1,
       "style": "IPY_MODEL_e6d87d40f39541de927291cd61c3c580",
       "value": 1
      }
     },
     "36169023789445ef9f05be6c551038e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37cf3928993c4164910ad1b13a196a3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "39a8d271178e4bfaa9298f17ca4f5690": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ef2f5cb7c6764578b5d94f97d4aa330a",
       "style": "IPY_MODEL_7aaeaf608f0e4f24a2298fa021b70771",
       "value": " 1/1 [12:50&lt;00:00, 770.63s/it]"
      }
     },
     "3c5b5c9150094648891e197413132184": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4b4471bc051c41c09d6a591f95afe3d8",
        "IPY_MODEL_625fc764ad174e9ca193de144f97598f",
        "IPY_MODEL_102f49e793614c15aa250675e01c245a"
       ],
       "layout": "IPY_MODEL_c7a6b91a7f6f4649a20fb73923162668"
      }
     },
     "3d05a31be31e4e199307629f99e2bfc6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "425527c57c4149c1a23dee845f3f93ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b4471bc051c41c09d6a591f95afe3d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_98055897c776414ca08f98000712e0ed",
       "style": "IPY_MODEL_5bf193c37f1d46bf9baf5002af04f8d4",
       "value": "100%"
      }
     },
     "5b8a5042c03042d68b3781fdfb4e6f52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5bf193c37f1d46bf9baf5002af04f8d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5e3c8cd0374c4b78bdfd0634762c5bfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_14626b858c814dc1a8171732859c869b",
       "max": 1,
       "style": "IPY_MODEL_045f30c43ee04a18acd4a6f8719c471b"
      }
     },
     "5f028a21200b4959861232426ea931e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "625fc764ad174e9ca193de144f97598f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a6c513d267144709b6db62d8b73e13be",
       "max": 1,
       "style": "IPY_MODEL_af9533e669de4da1bbed014592bb78a5",
       "value": 1
      }
     },
     "62d41b48e329428682fa394c5585828e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_425527c57c4149c1a23dee845f3f93ee",
       "style": "IPY_MODEL_9954b50a6759423b8bd7249a281f2317",
       "value": "  0%"
      }
     },
     "7aaeaf608f0e4f24a2298fa021b70771": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "89ecaee13516488bab04586e30247be6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_62d41b48e329428682fa394c5585828e",
        "IPY_MODEL_5e3c8cd0374c4b78bdfd0634762c5bfd",
        "IPY_MODEL_baf25ff5b1b64730b1bb62678d6212f0"
       ],
       "layout": "IPY_MODEL_37cf3928993c4164910ad1b13a196a3b"
      }
     },
     "8f5ebb7e70494b008a72768438eb7d92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "95f194c62a55420183e56dcb1cd44965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "98055897c776414ca08f98000712e0ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9954b50a6759423b8bd7249a281f2317": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d6101bea4c94000a55a8aec3a0c696a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_95f194c62a55420183e56dcb1cd44965",
       "max": 1,
       "style": "IPY_MODEL_f2afc55e63654a1892e69a72dbbcc4a1",
       "value": 1
      }
     },
     "a022830d13f84fe2aa1a6fd9a285f122": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6c513d267144709b6db62d8b73e13be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a887b64077b043f1a069dce02394916b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5f028a21200b4959861232426ea931e4",
       "style": "IPY_MODEL_11546c4ee3584559a99a9fd15044b97d",
       "value": "100%"
      }
     },
     "af9533e669de4da1bbed014592bb78a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b1f22d4ea134435fb0cb44989e9968d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "baf25ff5b1b64730b1bb62678d6212f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e1654ce311e043349f4d2bbf74a8b532",
       "style": "IPY_MODEL_b1f22d4ea134435fb0cb44989e9968d2",
       "value": " 0/1 [00:05&lt;?, ?it/s]"
      }
     },
     "c1701c46daf44a0ea73a790c221c8ff5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_36169023789445ef9f05be6c551038e9",
       "style": "IPY_MODEL_ef69abbfcdfb4132b6a6787d18a743f2",
       "value": "100%"
      }
     },
     "c7a6b91a7f6f4649a20fb73923162668": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d9240bf8f83049ff856de45a10565d15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a022830d13f84fe2aa1a6fd9a285f122",
       "style": "IPY_MODEL_e27a66927b7a4375bc7dbeb7f0f202bb",
       "value": " 1/1 [00:17&lt;00:00, 17.64s/it]"
      }
     },
     "dd63bd5b54684233a8233ab8221a09b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e1654ce311e043349f4d2bbf74a8b532": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e27a66927b7a4375bc7dbeb7f0f202bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e57d1d8422b94ffc808c96f405f1c56a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6d87d40f39541de927291cd61c3c580": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e7066aadd88048b885fe1243ce0b8724": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c1701c46daf44a0ea73a790c221c8ff5",
        "IPY_MODEL_9d6101bea4c94000a55a8aec3a0c696a",
        "IPY_MODEL_d9240bf8f83049ff856de45a10565d15"
       ],
       "layout": "IPY_MODEL_3d05a31be31e4e199307629f99e2bfc6"
      }
     },
     "ef2f5cb7c6764578b5d94f97d4aa330a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ef69abbfcdfb4132b6a6787d18a743f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f2afc55e63654a1892e69a72dbbcc4a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
